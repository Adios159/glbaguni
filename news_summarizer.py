#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë‰´ìŠ¤ ìš”ì•½ ì‹œìŠ¤í…œ
ì‚¬ìš©ìê°€ ìì—°ì–´ë¡œ ë‰´ìŠ¤ ì£¼ì œë¥¼ ì…ë ¥í•˜ë©´, í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³  ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì•„ ìš”ì•½í•˜ëŠ” ì‹œìŠ¤í…œ
"""

import requests
import feedparser
import json
import re
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from urllib.parse import urljoin, urlparse
import logging
from dataclasses import dataclass

try:
    from newspaper import Article
    NEWSPAPER_AVAILABLE = True
except ImportError:
    NEWSPAPER_AVAILABLE = False
    # Create a dummy BeautifulSoup import to avoid errors
    try:
        from bs4 import BeautifulSoup
    except ImportError:
        BeautifulSoup = None

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
    LEGACY_OPENAI = False
except ImportError:
    try:
        import openai
        OPENAI_AVAILABLE = True
        LEGACY_OPENAI = True
    except ImportError:
        OPENAI_AVAILABLE = False
        LEGACY_OPENAI = False
        # Create dummy openai module to avoid import errors
        openai = None

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class NewsArticle:
    title: str
    link: str
    summary: str
    content: str = ""
    published_date: Optional[str] = None
    source: str = ""

class NewsKeywordExtractor:
    """í‚¤ì›Œë“œ ì¶”ì¶œ í´ë˜ìŠ¤"""
    
    def __init__(self, openai_api_key: Optional[str] = None):
        self.openai_api_key = openai_api_key
        self.openai_client = None
        if openai_api_key and OPENAI_AVAILABLE:
            if LEGACY_OPENAI:
                import openai
                openai.api_key = openai_api_key
            else:
                self.openai_client = OpenAI(api_key=openai_api_key)
    
    def extract_keywords_with_gpt(self, text: str) -> List[str]:
        """GPTë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if not self.openai_api_key or not OPENAI_AVAILABLE:
            logger.warning("OpenAI API key not provided or openai package not installed. Using simple keyword extraction.")
            return self.extract_keywords_simple(text)
        
        try:
            prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ë‰´ìŠ¤ ê²€ìƒ‰ì— ìœ ìš©í•œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
- ê³ ìœ ëª…ì‚¬(íšŒì‚¬ëª…, ì¸ë¬¼ëª…, ì§€ì—­ëª…, ê¸°ìˆ ëª… ë“±)ë¥¼ ìš°ì„  ì¶”ì¶œ
- í•µì‹¬ ì£¼ì œì–´ë¥¼ í¬í•¨
- ìµœëŒ€ 10ê°œê¹Œì§€
- ê° í‚¤ì›Œë“œëŠ” ë”°ì˜´í‘œ ì—†ì´ ì½¤ë§ˆë¡œ êµ¬ë¶„
- í‚¤ì›Œë“œë§Œ ì¶œë ¥í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”

í…ìŠ¤íŠ¸: "{text}"

í‚¤ì›Œë“œ:"""

            if LEGACY_OPENAI:
                import openai
                response = openai.ChatCompletion.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ë‰´ìŠ¤ í‚¤ì›Œë“œ ì¶”ì¶œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=200,
                    temperature=0.3
                )
                keywords_text = response.choices[0].message.content.strip()
            else:
                response = self.openai_client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ë‰´ìŠ¤ í‚¤ì›Œë“œ ì¶”ì¶œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=200,
                    temperature=0.3
                )
                keywords_text = response.choices[0].message.content.strip()
            
            keywords = [kw.strip() for kw in keywords_text.split(',') if kw.strip()]
            logger.info(f"GPTë¡œ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords}")
            return keywords[:10]  # ìµœëŒ€ 10ê°œ
            
        except Exception as e:
            logger.error(f"GPT í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return self.extract_keywords_simple(text)
    
    def extract_keywords_simple(self, text: str) -> List[str]:
        """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (GPT ì‚¬ìš© ë¶ˆê°€ì‹œ ëŒ€ì•ˆ)"""
        # í•œêµ­ì–´ í‚¤ì›Œë“œ íŒ¨í„´ ë§¤ì¹­
        keywords = []
        
        # ê¸°ë³¸ì ì¸ í‚¤ì›Œë“œ íŒ¨í„´ë“¤
        patterns = {
            'íšŒì‚¬ëª…': r'(ì‚¼ì„±|LG|SK|í˜„ëŒ€|ê¸°ì•„|ë„¤ì´ë²„|ì¹´ì¹´ì˜¤|ì¿ íŒ¡|ë°°ë‹¬ì˜ë¯¼ì¡±|í† ìŠ¤|TSMC|ì• í”Œ|êµ¬ê¸€|ë§ˆì´í¬ë¡œì†Œí”„íŠ¸|í…ŒìŠ¬ë¼)',
            'ê¸°ìˆ ': r'(ë°˜ë„ì²´|AI|ì¸ê³µì§€ëŠ¥|5G|6G|ë¸”ë¡ì²´ì¸|ë©”íƒ€ë²„ìŠ¤|NFT|í´ë¼ìš°ë“œ|ë¹…ë°ì´í„°)',
            'ê²½ì œ': r'(ì£¼ê°€|ì¦ì‹œ|ì½”ìŠ¤í”¼|ë‚˜ìŠ¤ë‹¥|ë‹¬ëŸ¬|ì›í™”|ê¸ˆë¦¬|ì¸í”Œë ˆì´ì…˜|ê²½ê¸°ì¹¨ì²´)',
            'ì •ì¹˜': r'(ëŒ€í†µë ¹|êµ­íšŒ|ì •ë¶€|ì—¬ë‹¹|ì•¼ë‹¹|ì„ ê±°|ì •ì±…|ë²•ì•ˆ)',
            'ì‚¬íšŒ': r'(ì½”ë¡œë‚˜|ë°±ì‹ |ê¸°í›„|í™˜ê²½|êµìœ¡|ì˜ë£Œ|ë³µì§€)'
        }
        
        for category, pattern in patterns.items():
            matches = re.findall(pattern, text, re.IGNORECASE)
            keywords.extend(matches)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        keywords = list(set(keywords))
        logger.info(f"ê°„ë‹¨ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords}")
        return keywords[:10]

class NewsContentParser:
    """ë‰´ìŠ¤ ë³¸ë¬¸ íŒŒì‹± í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def parse_article_content(self, url: str) -> str:
        """ê¸°ì‚¬ ë³¸ë¬¸ íŒŒì‹±"""
        try:
            if NEWSPAPER_AVAILABLE:
                return self._parse_with_newspaper(url)
            else:
                return self._parse_with_beautifulsoup(url)
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ íŒŒì‹± ì‹¤íŒ¨ ({url}): {e}")
            return ""
    
    def _parse_with_newspaper(self, url: str) -> str:
        """newspaper3kë¥¼ ì‚¬ìš©í•œ íŒŒì‹±"""
        try:
            article = Article(url, language='ko')
            article.download()
            article.parse()
            return article.text
        except Exception as e:
            logger.error(f"Newspaper íŒŒì‹± ì‹¤íŒ¨: {e}")
            return self._parse_with_beautifulsoup(url)
    
    def _parse_with_beautifulsoup(self, url: str) -> str:
        """BeautifulSoupì„ ì‚¬ìš©í•œ íŒŒì‹±"""
        if not BeautifulSoup:
            logger.error("BeautifulSoup not available. Please install beautifulsoup4.")
            return ""
            
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì¼ë°˜ì ì¸ ê¸°ì‚¬ ë³¸ë¬¸ ì„ íƒìë“¤
            selectors = [
                'article',
                '.article-content',
                '.news-content',
                '.post-content',
                '#articleText',
                '.article_view',
                '.read_body'
            ]
            
            content = ""
            for selector in selectors:
                elements = soup.select(selector)
                if elements:
                    content = ' '.join([elem.get_text().strip() for elem in elements])
                    break
            
            # ì„ íƒìë¡œ ì°¾ì§€ ëª»í•œ ê²½ìš° p íƒœê·¸ë“¤ì„ ìˆ˜ì§‘
            if not content:
                paragraphs = soup.find_all('p')
                content = ' '.join([p.get_text().strip() for p in paragraphs if len(p.get_text().strip()) > 50])
            
            return content
        except Exception as e:
            logger.error(f"BeautifulSoup íŒŒì‹± ì‹¤íŒ¨: {e}")
            return ""

class NewsSummarizer:
    """ë‰´ìŠ¤ ìš”ì•½ í´ë˜ìŠ¤"""
    
    def __init__(self, openai_api_key: Optional[str] = None):
        self.openai_api_key = openai_api_key
        self.openai_client = None
        if openai_api_key and OPENAI_AVAILABLE:
            if LEGACY_OPENAI:
                import openai
                openai.api_key = openai_api_key
            else:
                self.openai_client = OpenAI(api_key=openai_api_key)
    
    def summarize_article(self, content: str, title: str = "") -> str:
        """ê¸°ì‚¬ ìš”ì•½"""
        if not content.strip():
            return "ë³¸ë¬¸ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        if not self.openai_api_key or not OPENAI_AVAILABLE:
            return self._simple_summarize(content)
        
        try:
            prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ 3-4ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”:

ì œëª©: {title}
ë³¸ë¬¸: {content[:2000]}  # í† í° ì œí•œì„ ìœ„í•´ ì•ë¶€ë¶„ë§Œ

ìš”ì•½:"""

            if LEGACY_OPENAI:
                import openai
                response = openai.ChatCompletion.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ë‰´ìŠ¤ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•µì‹¬ ë‚´ìš©ì„ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=300,
                    temperature=0.3
                )
                summary = response.choices[0].message.content.strip()
            else:
                response = self.openai_client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ë‰´ìŠ¤ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•µì‹¬ ë‚´ìš©ì„ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=300,
                    temperature=0.3
                )
                summary = response.choices[0].message.content.strip()
            
            return summary
            
        except Exception as e:
            logger.error(f"GPT ìš”ì•½ ì‹¤íŒ¨: {e}")
            return self._simple_summarize(content)
    
    def _simple_summarize(self, content: str) -> str:
        """ê°„ë‹¨í•œ ìš”ì•½ (GPT ì‚¬ìš© ë¶ˆê°€ì‹œ)"""
        sentences = re.split(r'[.!?]', content)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 20]
        
        # ì•ì˜ 3ë¬¸ì¥ ì •ë„ë¥¼ ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©
        summary_sentences = sentences[:3]
        return ' '.join(summary_sentences) + '.'

class NewsAggregator:
    """ë‰´ìŠ¤ ìˆ˜ì§‘ ë° í†µí•© í´ë˜ìŠ¤"""
    
    def __init__(self, openai_api_key: Optional[str] = None):
        self.keyword_extractor = NewsKeywordExtractor(openai_api_key)
        self.content_parser = NewsContentParser()
        self.summarizer = NewsSummarizer(openai_api_key)
        self.rss_feeds = self._get_rss_feeds()
    
    def _get_rss_feeds(self) -> Dict[str, List[str]]:
        """RSS í”¼ë“œ ëª©ë¡"""
        rss_feeds = {
            "SBS": [
                "https://news.sbs.co.kr/news/headlineRssFeed.do?plink=RSSREADER",
                "https://news.sbs.co.kr/news/newsflashRssFeed.do?plink=RSSREADER",
                "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=01",  # ì •ì¹˜
                "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=02",  # ê²½ì œ
                "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=03",  # ì‚¬íšŒ
                "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=07",  # ë¬¸í™”
                "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=08",  # êµ­ì œ
                "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=14",  # ì—°ì˜ˆ
                "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=09",  # ìŠ¤í¬ì¸ 
            ],
            "JTBC": [
                "https://news-ex.jtbc.co.kr/v1/get/rss/newsflesh",
                "https://news-ex.jtbc.co.kr/v1/get/rss/issue",
                "https://news-ex.jtbc.co.kr/v1/get/rss/section/10",  # ì •ì¹˜
                "https://news-ex.jtbc.co.kr/v1/get/rss/section/20",  # ê²½ì œ
                "https://news-ex.jtbc.co.kr/v1/get/rss/section/30",  # ì‚¬íšŒ
                "https://news-ex.jtbc.co.kr/v1/get/rss/section/40",  # ë¬¸í™”
                "https://news-ex.jtbc.co.kr/v1/get/rss/section/50",  # êµ­ì œ
                "https://news-ex.jtbc.co.kr/v1/get/rss/section/60",  # ìŠ¤í¬ì¸ 
            ],
            "Yonhap": [
                "https://www.yna.co.kr/rss/news.xml",
                "https://www.yna.co.kr/rss/politics.xml",
                "https://www.yna.co.kr/rss/economy.xml",
                "https://www.yna.co.kr/rss/industry.xml",
                "https://www.yna.co.kr/rss/society.xml",
                "https://www.yna.co.kr/rss/international.xml",
                "https://www.yna.co.kr/rss/culture.xml",
                "https://www.yna.co.kr/rss/entertainment.xml",
                "https://www.yna.co.kr/rss/sports.xml",
            ],
            "MaeilEconomy": [
                "https://www.mk.co.kr/rss/30000001/",
                "https://www.mk.co.kr/rss/40300001/",
                "https://www.mk.co.kr/rss/30200030/",
                "https://www.mk.co.kr/rss/30100041/",
                "https://www.mk.co.kr/rss/30800011/",
                "https://www.mk.co.kr/rss/30300018/",
                "https://www.mk.co.kr/rss/50400012/",
                "https://www.mk.co.kr/rss/50700001/",
            ],
            "Korean_Government": {
                "êµ­ë¬´ì¡°ì •ì‹¤": "https://www.korea.kr/rss/dept_opm.xml",
                "ê¸°íšì¬ì •ë¶€": "https://www.korea.kr/rss/dept_moef.xml",
                "êµìœ¡ë¶€": "https://www.korea.kr/rss/dept_moe.xml",
                "ê³¼í•™ê¸°ìˆ ì •ë³´í†µì‹ ë¶€": "https://www.korea.kr/rss/dept_msit.xml",
                "ì™¸êµë¶€": "https://www.korea.kr/rss/dept_mofa.xml",
                "í†µì¼ë¶€": "https://www.korea.kr/rss/dept_unikorea.xml",
                "ë²•ë¬´ë¶€": "https://www.korea.kr/rss/dept_moj.xml",
                "êµ­ë°©ë¶€": "https://www.korea.kr/rss/dept_mnd.xml",
                "í–‰ì •ì•ˆì „ë¶€": "https://www.korea.kr/rss/dept_mois.xml",
                "ë³´í›ˆì²˜": "https://www.korea.kr/rss/dept_mpva.xml",
                "ë¬¸í™”ì²´ìœ¡ê´€ê´‘ë¶€": "https://www.korea.kr/rss/dept_mcst.xml",
                "ë†ë¦¼ì¶•ì‚°ì‹í’ˆë¶€": "https://www.korea.kr/rss/dept_mafra.xml",
                "ì‚°ì—…í†µìƒìì›ë¶€": "https://www.korea.kr/rss/dept_motie.xml",
                "ë³´ê±´ë³µì§€ë¶€": "https://www.korea.kr/rss/dept_mw.xml",
                "í™˜ê²½ë¶€": "https://www.korea.kr/rss/dept_me.xml",
                "ê³ ìš©ë…¸ë™ë¶€": "https://www.korea.kr/rss/dept_moel.xml",
                "ì—¬ì„±ê°€ì¡±ë¶€": "https://www.korea.kr/rss/dept_mogef.xml",
                "êµ­í† êµí†µë¶€": "https://www.korea.kr/rss/dept_molit.xml",
                "í•´ì–‘ìˆ˜ì‚°ë¶€": "https://www.korea.kr/rss/dept_mof.xml",
                "ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€": "https://www.korea.kr/rss/dept_mss.xml",
                "ì¸ì‚¬í˜ì‹ ì²˜": "https://www.korea.kr/rss/dept_mpm.xml",
                "ë²•ì œì²˜": "https://www.korea.kr/rss/dept_moleg.xml",
                "ì‹í’ˆì˜ì•½í’ˆì•ˆì „ì²˜": "https://www.korea.kr/rss/dept_mfds.xml",
                "êµ­ì„¸ì²­": "https://www.korea.kr/rss/dept_nts.xml",
                "ê´€ì„¸ì²­": "https://www.korea.kr/rss/dept_customs.xml",
                "ì¡°ë‹¬ì²­": "https://www.korea.kr/rss/dept_pps.xml",
                "í†µê³„ì²­": "https://www.korea.kr/rss/dept_kostat.xml",
                "í•­ê³µìš°ì£¼ì—°êµ¬ì›": "https://www.korea.kr/rss/dept_kasa.xml",
                "ì •ë¶€ë²•ë¬´ê³µë‹¨": "https://www.korea.kr/rss/dept_oka.xml",
                "ìŠ¤í¬ì¸ ìœ¤ë¦¬ì„¼í„°": "https://www.korea.kr/rss/dept_spo.xml",
                "ë³‘ë¬´ì²­": "https://www.korea.kr/rss/dept_mma.xml",
                "ë°©ìœ„ì‚¬ì—…ì²­": "https://www.korea.kr/rss/dept_dapa.xml",
                "ê²½ì°°ì²­": "https://www.korea.kr/rss/dept_npa.xml",
                "ì†Œë°©ì²­": "https://www.korea.kr/rss/dept_nfa.xml",
                "í•´ì–‘ê²½ì°°ì²­": "https://www.korea.kr/rss/dept_kcg.xml",
                "í•œêµ­ë³´í›ˆë³µì§€ì˜ë£Œê³µë‹¨": "https://www.korea.kr/rss/dept_khs.xml",
                "ë†ì´Œì§„í¥ì²­": "https://www.korea.kr/rss/dept_rda.xml",
                "ì‚°ë¦¼ì²­": "https://www.korea.kr/rss/dept_forest.xml",
                "íŠ¹í—ˆì²­": "https://www.korea.kr/rss/dept_kipo.xml",
                "ì§ˆë³‘ê´€ë¦¬ì²­": "https://www.korea.kr/rss/dept_kdca.xml",
                "ê¸°ìƒì²­": "https://www.korea.kr/rss/dept_kma.xml",
                "ì•„ì‹œì•„ë¬¸í™”ì „ë‹¹": "https://www.korea.kr/rss/dept_macc.xml",
                "ì‚¬í–‰ì‚°ì—…í†µí•©ê°ë…ìœ„ì›íšŒ": "https://www.korea.kr/rss/dept_sda.xml"
            }
        }
        return rss_feeds
    
    def fetch_rss_articles(self, rss_url: str, source: str) -> List[Dict]:
        """RSSì—ì„œ ê¸°ì‚¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        try:
            feed = feedparser.parse(rss_url)
            articles = []
            
            for entry in feed.entries:
                article = {
                    'title': entry.get('title', ''),
                    'link': entry.get('link', ''),
                    'summary': entry.get('summary', ''),
                    'published': entry.get('published', ''),
                    'source': source
                }
                articles.append(article)
            
            logger.info(f"RSSì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘: {source}")
            return articles
            
        except Exception as e:
            logger.error(f"RSS ìˆ˜ì§‘ ì‹¤íŒ¨ ({rss_url}): {e}")
            return []
    
    def filter_articles_by_keywords(self, articles: List[Dict], keywords: List[str]) -> List[Dict]:
        """í‚¤ì›Œë“œë¡œ ê¸°ì‚¬ í•„í„°ë§"""
        if not keywords:
            return articles
        
        filtered_articles = []
        
        for article in articles:
            title = article.get('title', '').lower()
            summary = article.get('summary', '').lower()
            
            # í‚¤ì›Œë“œ ì¤‘ í•˜ë‚˜ë¼ë„ ì œëª©ì´ë‚˜ ìš”ì•½ì— í¬í•¨ë˜ë©´ ì„ íƒ
            for keyword in keywords:
                if keyword.lower() in title or keyword.lower() in summary:
                    filtered_articles.append(article)
                    break
        
        logger.info(f"í‚¤ì›Œë“œ í•„í„°ë§ ê²°ê³¼: {len(filtered_articles)}ê°œ ê¸°ì‚¬")
        return filtered_articles
    
    def process_news_query(self, query: str, max_articles: int = 10) -> List[NewsArticle]:
        """ë‰´ìŠ¤ ì¿¼ë¦¬ ì²˜ë¦¬ (ë©”ì¸ í•¨ìˆ˜)"""
        logger.info(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘: '{query}'")
        
        # 1. í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self.keyword_extractor.extract_keywords_with_gpt(query)
        if not keywords:
            logger.warning("í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # 2. RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘
        all_articles = []
        
        for source, feeds in self.rss_feeds.items():
            if isinstance(feeds, dict):  # ì •ë¶€ RSSì˜ ê²½ìš°
                for dept_name, feed_url in feeds.items():
                    articles = self.fetch_rss_articles(feed_url, f"{source}_{dept_name}")
                    all_articles.extend(articles)
            else:  # ì¼ë°˜ ì–¸ë¡ ì‚¬ RSSì˜ ê²½ìš°
                for feed_url in feeds:
                    articles = self.fetch_rss_articles(feed_url, source)
                    all_articles.extend(articles)
                    time.sleep(0.5)  # API í˜¸ì¶œ ì œí•œ ê³ ë ¤
        
        # 3. í‚¤ì›Œë“œë¡œ í•„í„°ë§
        filtered_articles = self.filter_articles_by_keywords(all_articles, keywords)
        
        # 4. ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        unique_articles = {}
        for article in filtered_articles:
            url = article['link']
            if url not in unique_articles:
                unique_articles[url] = article
        
        filtered_articles = list(unique_articles.values())[:max_articles]
        
        # 5. ë³¸ë¬¸ í¬ë¡¤ë§ ë° ìš”ì•½
        result_articles = []
        
        for i, article in enumerate(filtered_articles):
            logger.info(f"ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ({i+1}/{len(filtered_articles)}): {article['title']}")
            
            # ë³¸ë¬¸ í¬ë¡¤ë§
            content = self.content_parser.parse_article_content(article['link'])
            
            # ìš”ì•½ ìƒì„±
            summary = self.summarizer.summarize_article(content, article['title'])
            
            news_article = NewsArticle(
                title=article['title'],
                link=article['link'],
                summary=summary,
                content=content[:500] + "..." if len(content) > 500 else content,
                published_date=article.get('published'),
                source=article['source']
            )
            
            result_articles.append(news_article)
            time.sleep(1)  # API í˜¸ì¶œ ì œí•œ ê³ ë ¤
        
        logger.info(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì™„ë£Œ: {len(result_articles)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬")
        return result_articles

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # OpenAI API í‚¤ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ë‚˜ ì§ì ‘ ì…ë ¥)
    import os
    openai_api_key = os.getenv('OPENAI_API_KEY')  # í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
    
    if not openai_api_key:
        openai_api_key = input("OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì„ íƒì‚¬í•­, ì—”í„°ë¡œ ê±´ë„ˆë›°ê¸°): ").strip()
        if not openai_api_key:
            openai_api_key = None
    
    # ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
    aggregator = NewsAggregator(openai_api_key)
    
    # ì‚¬ìš©ì ì…ë ¥
    while True:
        query = input("\në‰´ìŠ¤ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: quit): ").strip()
        
        if query.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
            break
        
        if not query:
            continue
        
        try:
            # ë‰´ìŠ¤ ê²€ìƒ‰ ë° ìš”ì•½
            articles = aggregator.process_news_query(query, max_articles=5)
            
            if not articles:
                print("ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"\nğŸ“° '{query}' ê´€ë ¨ ë‰´ìŠ¤ ìš”ì•½ ({len(articles)}ê°œ)")
            print("=" * 80)
            
            for i, article in enumerate(articles, 1):
                print(f"\n{i}. {article.title}")
                print(f"   ì¶œì²˜: {article.source}")
                print(f"   ë§í¬: {article.link}")
                print(f"   ìš”ì•½: {article.summary}")
                print("-" * 40)
            
            # JSON í˜•íƒœë¡œë„ ì¶œë ¥
            print("\nğŸ“„ JSON í˜•íƒœ ê²°ê³¼:")
            result_json = []
            for article in articles:
                result_json.append({
                    "title": article.title,
                    "link": article.link,
                    "summary": article.summary,
                    "source": article.source
                })
            
            print(json.dumps(result_json, ensure_ascii=False, indent=2))
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

if __name__ == "__main__":
    main() 
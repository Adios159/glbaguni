#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë‰´ìŠ¤ ìš”ì•½ ì‹œìŠ¤í…œ
ì‚¬ìš©ìê°€ ìì—°ì–´ë¡œ ë‰´ìŠ¤ ì£¼ì œë¥¼ ì…ë ¥í•˜ë©´, í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³  ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì•„ ìš”ì•½í•˜ëŠ” ì‹œìŠ¤í…œ
"""

import json
import logging
import re
import time
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple, Union, cast
from urllib.parse import urljoin, urlparse

import feedparser
import requests

# ë³´ì•ˆ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from .security import create_safe_prompt, validate_api_key, validate_input

    SECURITY_AVAILABLE = True
except ImportError:
    SECURITY_AVAILABLE = False

try:
    from newspaper import Article

    NEWSPAPER_AVAILABLE = True
except ImportError:
    NEWSPAPER_AVAILABLE = False
    # Create a dummy BeautifulSoup import to avoid errors
    try:
        from bs4 import BeautifulSoup
    except ImportError:
        BeautifulSoup = None

try:
    from openai import OpenAI
    from openai.types.chat import ChatCompletionMessageParam

    OPENAI_AVAILABLE = True
    LEGACY_OPENAI = False
except ImportError:
    try:
        import openai  # type: ignore

        OPENAI_AVAILABLE = True
        LEGACY_OPENAI = True
        # For legacy OpenAI, we'll use Any for messages type
        ChatCompletionMessageParam = Any  # type: ignore
    except ImportError:
        OPENAI_AVAILABLE = False
        LEGACY_OPENAI = False
        # Create dummy openai module to avoid import errors
        openai = None  # type: ignore
        ChatCompletionMessageParam = Any  # type: ignore

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


@dataclass
class NewsArticle:
    title: str
    link: str
    summary: str
    content: str = ""
    published_date: Optional[str] = None
    source: str = ""


class NewsKeywordExtractor:
    """í‚¤ì›Œë“œ ì¶”ì¶œ í´ë˜ìŠ¤"""

    def __init__(self, openai_api_key: Optional[str] = None):
        self.openai_api_key = openai_api_key
        self.openai_client = None  # type: ignore
        if openai_api_key and OPENAI_AVAILABLE:
            if LEGACY_OPENAI:
                # Legacy OpenAI ì‚¬ìš©ì‹œì—ëŠ” ì „ì—­ ì„¤ì •
                if openai is not None:
                    openai.api_key = openai_api_key  # type: ignore
            else:
                self.openai_client = OpenAI(api_key=openai_api_key)  # type: ignore

    def extract_keywords_with_gpt(self, text: str) -> List[str]:
        """GPTë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ - ë³´ì•ˆ ê°•í™”"""
        if not self.openai_api_key or not OPENAI_AVAILABLE:
            logger.warning(
                "OpenAI API key not provided or openai package not installed. Using simple keyword extraction."
            )
            return self.extract_keywords_simple(text)

        # ë³´ì•ˆ ê²€ì¦: ì‚¬ìš©ì ì…ë ¥ ì •í™”
        if SECURITY_AVAILABLE:
            try:
                safe_text = validate_input(text, "query")
                logger.info(
                    f"ğŸ”’ ì‚¬ìš©ì ì…ë ¥ ê²€ì¦ ì™„ë£Œ: {len(text)} -> {len(safe_text)} ë¬¸ì"
                )
            except ValueError as e:
                logger.error(f"ğŸš¨ ìœ„í—˜í•œ ì…ë ¥ ê°ì§€: {e}")
                return self.extract_keywords_simple(text)
        else:
            safe_text = text[:200]  # ê¸°ë³¸ ê¸¸ì´ ì œí•œ

        try:
            # ì•ˆì „í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ì‚¬ìš©
            system_message = """ë‹¹ì‹ ì€ ë‰´ìŠ¤ í‚¤ì›Œë“œ ì¶”ì¶œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìê°€ ì œê³µí•œ í…ìŠ¤íŠ¸ì—ì„œ ë‰´ìŠ¤ ê²€ìƒ‰ì— ìœ ìš©í•œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
- ê³ ìœ ëª…ì‚¬(íšŒì‚¬ëª…, ì¸ë¬¼ëª…, ì§€ì—­ëª…, ê¸°ìˆ ëª… ë“±)ë¥¼ ìš°ì„  ì¶”ì¶œ
- í•µì‹¬ ì£¼ì œì–´ë¥¼ í¬í•¨
- ìµœëŒ€ 10ê°œê¹Œì§€
- ê° í‚¤ì›Œë“œëŠ” ë”°ì˜´í‘œ ì—†ì´ ì½¤ë§ˆë¡œ êµ¬ë¶„
- í‚¤ì›Œë“œë§Œ ì¶œë ¥í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”"""

            if SECURITY_AVAILABLE:
                prompt_data = create_safe_prompt(safe_text, system_message)
                messages = cast(Any, prompt_data["messages"])
                max_tokens = prompt_data["max_tokens"]
                temperature = prompt_data["temperature"]
            else:
                # í´ë°±: ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ êµ¬ì¡°
                messages = cast(
                    Any,
                    [
                        {"role": "system", "content": system_message},
                        {"role": "user", "content": f"í…ìŠ¤íŠ¸: {safe_text}"},
                    ],
                )
                max_tokens = 200
                temperature = 0.3

            if LEGACY_OPENAI and openai is not None:
                # Legacy OpenAI API ì‚¬ìš©
                response = openai.ChatCompletion.create(  # type: ignore
                    model="gpt-3.5-turbo",
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                )
                keywords_text = response.choices[0].message.content or ""
                keywords_text = keywords_text.strip()
            elif self.openai_client is not None:
                # ìƒˆë¡œìš´ OpenAI í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
                response = self.openai_client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=temperature,
                )
                keywords_text = response.choices[0].message.content or ""
                keywords_text = keywords_text.strip()
            else:
                logger.error("OpenAI client not properly initialized")
                return self.extract_keywords_simple(text)

            if not keywords_text:
                logger.warning("Empty response from GPT, using simple extraction")
                return self.extract_keywords_simple(text)

            keywords = [kw.strip() for kw in keywords_text.split(",") if kw.strip()]
            logger.info(f"GPTë¡œ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords}")
            return keywords[:10]  # ìµœëŒ€ 10ê°œ

        except Exception as e:
            logger.error(f"GPT í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return self.extract_keywords_simple(text)

    def extract_keywords_simple(self, text: str) -> List[str]:
        """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (GPT ì‚¬ìš© ë¶ˆê°€ì‹œ ëŒ€ì•ˆ)"""
        # í•œêµ­ì–´ í‚¤ì›Œë“œ íŒ¨í„´ ë§¤ì¹­
        keywords = []

        # ê¸°ë³¸ì ì¸ í‚¤ì›Œë“œ íŒ¨í„´ë“¤
        patterns = {
            "íšŒì‚¬ëª…": r"(ì‚¼ì„±|LG|SK|í˜„ëŒ€|ê¸°ì•„|ë„¤ì´ë²„|ì¹´ì¹´ì˜¤|ì¿ íŒ¡|ë°°ë‹¬ì˜ë¯¼ì¡±|í† ìŠ¤|TSMC|ì• í”Œ|êµ¬ê¸€|ë§ˆì´í¬ë¡œì†Œí”„íŠ¸|í…ŒìŠ¬ë¼)",
            "ê¸°ìˆ ": r"(ë°˜ë„ì²´|AI|ì¸ê³µì§€ëŠ¥|5G|6G|ë¸”ë¡ì²´ì¸|ë©”íƒ€ë²„ìŠ¤|NFT|í´ë¼ìš°ë“œ|ë¹…ë°ì´í„°)",
            "ê²½ì œ": r"(ì£¼ê°€|ì¦ì‹œ|ì½”ìŠ¤í”¼|ë‚˜ìŠ¤ë‹¥|ë‹¬ëŸ¬|ì›í™”|ê¸ˆë¦¬|ì¸í”Œë ˆì´ì…˜|ê²½ê¸°ì¹¨ì²´)",
            "ì •ì¹˜": r"(ëŒ€í†µë ¹|êµ­íšŒ|ì •ë¶€|ì—¬ë‹¹|ì•¼ë‹¹|ì„ ê±°|ì •ì±…|ë²•ì•ˆ)",
            "ì‚¬íšŒ": r"(ì½”ë¡œë‚˜|ë°±ì‹ |ê¸°í›„|í™˜ê²½|êµìœ¡|ì˜ë£Œ|ë³µì§€)",
        }

        for category, pattern in patterns.items():
            matches = re.findall(pattern, text, re.IGNORECASE)
            keywords.extend(matches)

        # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        keywords = list(set(keywords))
        logger.info(f"ê°„ë‹¨ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords}")
        return keywords[:10]


class NewsContentParser:
    """ë‰´ìŠ¤ ë³¸ë¬¸ íŒŒì‹± í´ë˜ìŠ¤"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            }
        )

    def parse_article_content(self, url: str) -> str:
        """ê¸°ì‚¬ ë³¸ë¬¸ íŒŒì‹±"""
        try:
            if NEWSPAPER_AVAILABLE:
                return self._parse_with_newspaper(url)
            else:
                return self._parse_with_beautifulsoup(url)
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ íŒŒì‹± ì‹¤íŒ¨ ({url}): {e}")
            return ""

    def _parse_with_newspaper(self, url: str) -> str:
        """newspaper3kë¥¼ ì‚¬ìš©í•œ íŒŒì‹±"""
        try:
            article = Article(url, language="ko")
            article.download()
            article.parse()
            return article.text
        except Exception as e:
            logger.error(f"Newspaper íŒŒì‹± ì‹¤íŒ¨: {e}")
            return self._parse_with_beautifulsoup(url)

    def _parse_with_beautifulsoup(self, url: str) -> str:
        """BeautifulSoupì„ ì‚¬ìš©í•œ íŒŒì‹±"""
        if not BeautifulSoup:
            logger.error("BeautifulSoup not available. Please install beautifulsoup4.")
            return ""

        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")

            # ì¼ë°˜ì ì¸ ê¸°ì‚¬ ë³¸ë¬¸ ì„ íƒìë“¤
            selectors = [
                "article",
                ".article-content",
                ".news-content",
                ".post-content",
                "#articleText",
                ".article_view",
                ".read_body",
            ]

            content = ""
            for selector in selectors:
                elements = soup.select(selector)
                if elements:
                    content = elements[0].get_text(strip=True)
                    break

            if not content:
                # í´ë°±: ëª¨ë“  p íƒœê·¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                paragraphs = soup.find_all("p")
                content = "\n".join([p.get_text(strip=True) for p in paragraphs])

            return content

        except Exception as e:
            logger.error(f"BeautifulSoup íŒŒì‹± ì‹¤íŒ¨: {e}")
            return ""


class NewsSummarizer:
    """ë‰´ìŠ¤ ìš”ì•½ í´ë˜ìŠ¤"""

    def __init__(self, openai_api_key: Optional[str] = None):
        self.openai_api_key = openai_api_key
        self.openai_client = None  # type: ignore
        if openai_api_key and OPENAI_AVAILABLE:
            if LEGACY_OPENAI:
                # Legacy OpenAI ì‚¬ìš©ì‹œì—ëŠ” ì „ì—­ ì„¤ì •
                if openai is not None:
                    openai.api_key = openai_api_key  # type: ignore
            else:
                self.openai_client = OpenAI(api_key=openai_api_key)  # type: ignore

    def summarize_article(self, content: str, title: str = "") -> str:
        """ê¸°ì‚¬ ìš”ì•½"""
        if not content:
            return "ë³¸ë¬¸ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."

        if not self.openai_api_key or not OPENAI_AVAILABLE:
            logger.warning(
                "OpenAI API key not provided or openai package not installed. Using simple summarization."
            )
            return self._simple_summarize(content)

        try:
            # ë³¸ë¬¸ì´ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸° (2000ì ì´ë‚´)
            if len(content) > 2000:
                content = content[:2000] + "..."

            prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ 3-4ì¤„ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
- í•µì‹¬ ë‚´ìš©ë§Œ í¬í•¨
- ê°ê´€ì ì´ê³  ì¤‘ë¦½ì ì¸ í†¤
- í•œêµ­ì–´ë¡œ ì‘ì„±

ì œëª©: {title}
ë³¸ë¬¸: {content}

ìš”ì•½:"""

            if LEGACY_OPENAI and openai is not None:
                # Legacy OpenAI API ì‚¬ìš©
                response = openai.ChatCompletion.create(  # type: ignore
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ë‰´ìŠ¤ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                        {"role": "user", "content": prompt},
                    ],
                    max_tokens=300,
                    temperature=0.3,
                )
                summary = response.choices[0].message.content or ""
                if summary:
                    summary = summary.strip()
                else:
                    summary = self._simple_summarize(content)
            elif self.openai_client is not None:
                # ìƒˆë¡œìš´ OpenAI í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
                summarizer_messages = cast(
                    Any,
                    [
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ë‰´ìŠ¤ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                        {"role": "user", "content": prompt},
                    ],
                )
                response = self.openai_client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=summarizer_messages,
                    max_tokens=300,
                    temperature=0.3,
                )
                summary = response.choices[0].message.content or ""
                if summary:
                    summary = summary.strip()
                else:
                    summary = self._simple_summarize(content)
            else:
                logger.error("OpenAI client not properly initialized")
                return self._simple_summarize(content)

            return summary

        except Exception as e:
            logger.error(f"GPT ìš”ì•½ ì‹¤íŒ¨: {e}")
            return self._simple_summarize(content)

    def _simple_summarize(self, content: str) -> str:
        """ê°„ë‹¨í•œ ìš”ì•½ (GPT ì‚¬ìš© ë¶ˆê°€ì‹œ ëŒ€ì•ˆ)"""
        sentences = content.split(".")
        if len(sentences) > 3:
            return ". ".join(sentences[:3]) + "."
        return content


class NewsAggregator:
    """ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ìš”ì•½ ë©”ì¸ í´ë˜ìŠ¤"""

    def __init__(self, openai_api_key: Optional[str] = None):
        self.keyword_extractor = NewsKeywordExtractor(openai_api_key)
        self.content_parser = NewsContentParser()
        self.summarizer = NewsSummarizer(openai_api_key)
        self.rss_feeds = self._get_rss_feeds()

    def _get_rss_feeds(self) -> Dict[str, List[str]]:
        """RSS í”¼ë“œ ëª©ë¡ ë°˜í™˜ - ë¬´í•œë¡œë”© ë°©ì§€ë¥¼ ìœ„í•´ ì œí•œëœ í”¼ë“œë§Œ ì‚¬ìš©"""
        # ë¬´í•œë¡œë”© ë°©ì§€ë¥¼ ìœ„í•´ ê°€ì¥ ì•ˆì •ì ì¸ ì£¼ìš” í”¼ë“œë§Œ ì„ íƒ
        rss_feeds = {
            "SBS": [
                "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=01",  # í—¤ë“œë¼ì¸
                "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=02",  # ì •ì¹˜
                "https://news.sbs.co.kr/news/SectionRssFeed.do?sectionId=03",  # ê²½ì œ
            ],
            "JTBC": [
                "https://fs.jtbc.co.kr/RSS/newsflash.xml",  # ë‰´ìŠ¤í”Œë˜ì‹œ
                "https://fs.jtbc.co.kr/RSS/politics.xml",  # ì •ì¹˜
                "https://fs.jtbc.co.kr/RSS/economy.xml",  # ê²½ì œ
            ],
            "ì—°í•©ë‰´ìŠ¤": [
                "https://www.yonhapnews.co.kr/rss/allheadlines.xml",  # ì „ì²´
                "https://www.yonhapnews.co.kr/rss/politics.xml",  # ì •ì¹˜
                "https://www.yonhapnews.co.kr/rss/economy.xml",  # ê²½ì œ
            ],
            # ì •ë¶€RSS ì œê±° - ë¬´í•œë¡œë”©ì˜ ì£¼ìš” ì›ì¸
        }
        return rss_feeds

    def fetch_rss_articles(self, rss_url: str, source: str) -> List[Dict[str, Any]]:
        """RSSì—ì„œ ê¸°ì‚¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° - íƒ€ì„ì•„ì›ƒ ì¶”ê°€"""
        try:
            # íƒ€ì„ì•„ì›ƒ ì¶”ê°€í•˜ì—¬ ë¬´í•œ ëŒ€ê¸° ë°©ì§€
            import signal

            def timeout_handler(signum: int, frame: Any) -> None:
                raise TimeoutError("RSS fetch timeout")

            # Windowsì—ì„œëŠ” signal.SIGALRMì´ ì§€ì›ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ requests timeout ì‚¬ìš©
            feed = feedparser.parse(rss_url)
            articles: List[Dict[str, Any]] = []

            # ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ ì œí•œ (ë¬´í•œë£¨í”„ ë°©ì§€)
            max_entries = min(len(feed.entries), 20)  # ìµœëŒ€ 20ê°œë¡œ ì œí•œ

            for entry in feed.entries[:max_entries]:
                article = {
                    "title": entry.get("title", ""),
                    "link": entry.get("link", ""),
                    "summary": entry.get("summary", ""),
                    "published": entry.get("published", ""),
                    "source": source,
                }
                articles.append(article)

            logger.info(f"RSSì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘: {source}")
            return articles

        except Exception as e:
            logger.error(f"RSS ìˆ˜ì§‘ ì‹¤íŒ¨ ({rss_url}): {e}")
            return []

    def filter_articles_by_keywords(
        self, articles: List[Dict[str, Any]], keywords: List[str]
    ) -> List[Dict[str, Any]]:
        """í‚¤ì›Œë“œë¡œ ê¸°ì‚¬ í•„í„°ë§"""
        if not keywords:
            return articles

        filtered_articles = []

        for article in articles:
            title = article.get("title", "").lower()
            summary = article.get("summary", "").lower()

            # í‚¤ì›Œë“œ ì¤‘ í•˜ë‚˜ë¼ë„ ì œëª©ì´ë‚˜ ìš”ì•½ì— í¬í•¨ë˜ë©´ ì„ íƒ
            for keyword in keywords:
                if keyword.lower() in title or keyword.lower() in summary:
                    filtered_articles.append(article)
                    break

        logger.info(f"í‚¤ì›Œë“œ í•„í„°ë§ ê²°ê³¼: {len(filtered_articles)}ê°œ ê¸°ì‚¬")
        return filtered_articles

    def process_news_query(
        self, query: str, max_articles: int = 10
    ) -> Tuple[List[NewsArticle], List[str]]:
        """ë‰´ìŠ¤ ì¿¼ë¦¬ ì²˜ë¦¬ (ë©”ì¸ í•¨ìˆ˜) - í‚¤ì›Œë“œë„ í•¨ê»˜ ë°˜í™˜ - ë¬´í•œë¡œë”© ë°©ì§€"""
        logger.info(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘: '{query}'")

        # 1. í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self.keyword_extractor.extract_keywords_with_gpt(query)
        if not keywords:
            logger.warning("í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return [], []

        # 2. RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ - ì œí•œëœ ìˆ˜ì˜ í”¼ë“œë§Œ ì²˜ë¦¬
        all_articles: List[Dict[str, Any]] = []
        processed_feeds = 0
        max_feeds_per_source = 2  # ì†ŒìŠ¤ë‹¹ ìµœëŒ€ 2ê°œ í”¼ë“œë§Œ ì²˜ë¦¬

        for source, feeds in self.rss_feeds.items():
            feed_count = 0
            if isinstance(feeds, list):  # ì¼ë°˜ ì–¸ë¡ ì‚¬ RSSì˜ ê²½ìš°
                for feed_url in feeds:
                    if feed_count >= max_feeds_per_source:
                        break
                    try:
                        articles = self.fetch_rss_articles(feed_url, source)
                        all_articles.extend(articles)
                        feed_count += 1
                        processed_feeds += 1

                        # ë¬´í•œë¡œë”© ë°©ì§€: ë„ˆë¬´ ë§ì€ í”¼ë“œ ì²˜ë¦¬í•˜ì§€ ì•Šê¸°
                        if processed_feeds >= 6:  # ìµœëŒ€ 6ê°œ í”¼ë“œë§Œ ì²˜ë¦¬
                            logger.info(
                                f"ìµœëŒ€ í”¼ë“œ ìˆ˜ ë„ë‹¬, ì²˜ë¦¬ ì¤‘ë‹¨: {processed_feeds}"
                            )
                            break

                        time.sleep(0.2)  # API í˜¸ì¶œ ì œí•œ ê³ ë ¤ (0.5ì—ì„œ 0.2ë¡œ ë‹¨ì¶•)
                    except Exception as e:
                        logger.error(f"RSS ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        continue

            if processed_feeds >= 6:
                break

        logger.info(f"ì´ {processed_feeds}ê°œ í”¼ë“œì—ì„œ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")

        # 3. í‚¤ì›Œë“œë¡œ í•„í„°ë§
        filtered_articles = self.filter_articles_by_keywords(all_articles, keywords)

        # 4. ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        unique_articles: Dict[str, Dict[str, Any]] = {}
        for article in filtered_articles:
            url = article["link"]
            if url not in unique_articles:
                unique_articles[url] = article

        filtered_articles = list(unique_articles.values())[:max_articles]

        # 5. ë³¸ë¬¸ í¬ë¡¤ë§ ë° ìš”ì•½ - ì²˜ë¦¬ ì‹œê°„ ì œí•œ
        result_articles: List[NewsArticle] = []
        max_process_time = 30  # ìµœëŒ€ 30ì´ˆë§Œ ì²˜ë¦¬
        start_time = time.time()

        for i, article in enumerate(filtered_articles):
            # ì‹œê°„ ì´ˆê³¼ ì²´í¬
            if time.time() - start_time > max_process_time:
                logger.warning(f"ì²˜ë¦¬ ì‹œê°„ ì´ˆê³¼, {i}ê°œ ê¸°ì‚¬ë§Œ ì²˜ë¦¬ë¨")
                break

            logger.info(
                f"ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ({i+1}/{len(filtered_articles)}): {article['title']}"
            )

            try:
                # ë³¸ë¬¸ í¬ë¡¤ë§ - íƒ€ì„ì•„ì›ƒ ì¶”ê°€
                content = self.content_parser.parse_article_content(article["link"])

                # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ
                if len(content.strip()) < 100:
                    logger.warning(f"ë‚´ìš©ì´ ë„ˆë¬´ ì§§ì•„ ìŠ¤í‚µ: {article['title']}")
                    continue

                # ìš”ì•½ ìƒì„±
                summary = self.summarizer.summarize_article(content, article["title"])

                news_article = NewsArticle(
                    title=article["title"],
                    link=article["link"],
                    summary=summary,
                    content=content[:500] + "..." if len(content) > 500 else content,
                    published_date=article.get("published"),
                    source=article["source"],
                )

                result_articles.append(news_article)
                time.sleep(0.5)  # API í˜¸ì¶œ ì œí•œ ê³ ë ¤

            except Exception as e:
                logger.error(f"ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                continue

        logger.info(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì™„ë£Œ: {len(result_articles)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬")
        return result_articles, keywords

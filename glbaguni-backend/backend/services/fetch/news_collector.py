#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
News Collector Module
ë‰´ìŠ¤ ìˆ˜ì§‘ ë¡œì§ (200ì¤„ ì´í•˜)
"""

import logging
from datetime import datetime
from typing import List, Optional
from urllib.parse import urlparse

import chardet
import feedparser
import requests
from requests.adapters import HTTPAdapter

try:
    from ...models import Article
except ImportError:
    from models import Article

logger = logging.getLogger(__name__)


class NewsCollector:
    """ë‰´ìŠ¤ ìˆ˜ì§‘ ì „ë‹´ í´ë˜ìŠ¤"""

    def __init__(self):
        """ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”"""
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Glbaguni RSS Bot)",
                "Accept": "application/rss+xml, application/xml, text/xml, */*",
                "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
            }
        )
        self.session.mount("http://", HTTPAdapter(max_retries=3))
        self.session.mount("https://", HTTPAdapter(max_retries=3))

        logger.info("NewsCollector ì´ˆê¸°í™” ì™„ë£Œ")

    def fetch_rss_feed(
        self, rss_url: str, timeout: int = 10
    ) -> Optional[feedparser.FeedParserDict]:
        """
        RSS í”¼ë“œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

        Args:
            rss_url: RSS í”¼ë“œ URL
            timeout: íƒ€ì„ì•„ì›ƒ (ì´ˆ)

        Returns:
            íŒŒì‹±ëœ RSS í”¼ë“œ ë˜ëŠ” None
        """
        try:
            logger.info(f"ğŸ“¡ RSS í”¼ë“œ ìš”ì²­: {rss_url}")

            # HTTP ìš”ì²­ìœ¼ë¡œ RSS ê°€ì ¸ì˜¤ê¸°
            response = self.session.get(rss_url, timeout=timeout)
            response.raise_for_status()

            # ì¸ì½”ë”© ê°ì§€ ë° ë””ì½”ë”©
            encoding = self._detect_encoding(response)
            decoded_content = self._decode_content(response.content, encoding)

            # RSS íŒŒì‹±
            feed = feedparser.parse(decoded_content)

            # íŒŒì‹± ê²°ê³¼ ê²€ì¦
            if not hasattr(feed, "entries") or not feed.entries:
                logger.warning(f"âš ï¸ RSS í”¼ë“œì— ì—”íŠ¸ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {rss_url}")
                return None

            logger.info(f"âœ… RSS í”¼ë“œ íŒŒì‹± ì™„ë£Œ: {len(feed.entries)}ê°œ ì—”íŠ¸ë¦¬")
            return feed

        except requests.RequestException as e:
            logger.error(f"âŒ RSS í”¼ë“œ ìš”ì²­ ì‹¤íŒ¨ ({rss_url}): {e}")
            return None
        except Exception as e:
            logger.error(f"âŒ RSS í”¼ë“œ ì²˜ë¦¬ ì‹¤íŒ¨ ({rss_url}): {e}")
            return None

    def fetch_multiple_feeds(
        self, rss_urls: List[str], max_articles_per_feed: int = 10
    ) -> List[Article]:
        """
        ì—¬ëŸ¬ RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

        Args:
            rss_urls: RSS URL ë¦¬ìŠ¤íŠ¸
            max_articles_per_feed: í”¼ë“œë‹¹ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜

        Returns:
            ìˆ˜ì§‘ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        all_articles = []

        logger.info(f"ğŸ“š ë‹¤ì¤‘ RSS í”¼ë“œ ìˆ˜ì§‘ ì‹œì‘: {len(rss_urls)}ê°œ í”¼ë“œ")

        for i, rss_url in enumerate(rss_urls, 1):
            try:
                logger.info(f"ğŸ“„ [{i}/{len(rss_urls)}] ì²˜ë¦¬ ì¤‘: {rss_url}")

                # RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°
                feed = self.fetch_rss_feed(rss_url)
                if not feed:
                    continue

                # ê¸°ì‚¬ ë³€í™˜
                articles = self._convert_feed_to_articles(
                    feed, rss_url, max_articles_per_feed
                )
                all_articles.extend(articles)

                logger.info(
                    f"âœ… [{i}/{len(rss_urls)}] ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘"
                )

            except Exception as e:
                logger.error(f"âŒ RSS í”¼ë“œ ì²˜ë¦¬ ì‹¤íŒ¨ ({rss_url}): {e}")
                continue

        logger.info(f"ğŸ‰ ë‹¤ì¤‘ RSS ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(all_articles)}ê°œ ê¸°ì‚¬")
        return all_articles

    def _detect_encoding(self, response: requests.Response) -> str:
        """ì‘ë‹µì˜ ë¬¸ì ì¸ì½”ë”©ì„ ê°ì§€í•©ë‹ˆë‹¤."""
        # Content-Type í—¤ë”ì—ì„œ charset í™•ì¸
        content_type = response.headers.get("content-type", "").lower()
        if "charset=" in content_type:
            charset = content_type.split("charset=")[1].split(";")[0].strip()
            logger.debug(f"ğŸ” í—¤ë” ì¸ì½”ë”©: {charset}")
            return charset

        # chardetìœ¼ë¡œ ê°ì§€
        detected = chardet.detect(response.content)
        encoding = detected.get("encoding", "utf-8") if detected else "utf-8"
        confidence = detected.get("confidence", 0) if detected else 0

        logger.debug(f"ğŸ” ê°ì§€ëœ ì¸ì½”ë”©: {encoding} (ì‹ ë¢°ë„: {confidence:.2f})")

        # í•œêµ­ì–´ ì‚¬ì´íŠ¸ë¥¼ ìœ„í•œ ê¸°ë³¸ê°’
        if confidence < 0.7:
            return "utf-8"

        return encoding

    def _decode_content(self, content: bytes, encoding: str) -> str:
        """ë°”ì´íŠ¸ ì½˜í…ì¸ ë¥¼ ë¬¸ìì—´ë¡œ ë””ì½”ë”©í•©ë‹ˆë‹¤."""
        try:
            return content.decode(encoding, errors="ignore")
        except (UnicodeDecodeError, LookupError):
            logger.warning(f"âš ï¸ {encoding} ë””ì½”ë”© ì‹¤íŒ¨, UTF-8ë¡œ ëŒ€ì²´")
            return content.decode("utf-8", errors="ignore")

    def _convert_feed_to_articles(
        self, feed: feedparser.FeedParserDict, source_url: str, max_articles: int
    ) -> List[Article]:
        """RSS í”¼ë“œ ì—”íŠ¸ë¦¬ë¥¼ Article ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        articles = []
        source = urlparse(source_url).netloc or "unknown"

        for entry in feed.entries[:max_articles]:
            try:
                article = self._create_article_from_entry(entry, source)
                if article:
                    articles.append(article)
            except Exception as e:
                logger.debug(f"ì—”íŠ¸ë¦¬ ë³€í™˜ ì‹¤íŒ¨: {e}")
                continue

        return articles

    def _create_article_from_entry(self, entry, source: str) -> Optional[Article]:
        """RSS ì—”íŠ¸ë¦¬ì—ì„œ Article ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        if not hasattr(entry, "title") or not hasattr(entry, "link"):
            return None

        title = str(entry.title).strip()
        url = str(entry.link)

        if not title or not url.startswith(("http://", "https://")):
            return None

        # ì½˜í…ì¸  ì¶”ì¶œ
        content = self._extract_content_from_entry(entry)
        if not content or len(content.strip()) < 50:
            return None

        # ë°œí–‰ì¼ íŒŒì‹±
        published_date = self._parse_published_date(entry)

        return Article(
            title=title,
            url=url,
            content=content,
            published_date=published_date,
            author=getattr(entry, "author", None),
            source=source,
        )

    def _extract_content_from_entry(self, entry) -> str:
        """RSS ì—”íŠ¸ë¦¬ì—ì„œ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        # RSS í•„ë“œ ìš°ì„ ìˆœìœ„ ìˆœì„œë¡œ í™•ì¸
        content_fields = ["content", "summary", "description"]

        for field in content_fields:
            if hasattr(entry, field):
                field_content = getattr(entry, field)

                if isinstance(field_content, list) and field_content:
                    field_content = field_content[0]

                if isinstance(field_content, dict):
                    field_content = field_content.get("value", "") or field_content.get(
                        "content", ""
                    )

                if isinstance(field_content, str) and field_content.strip():
                    return self._clean_html_content(field_content)

        return ""

    def _clean_html_content(self, content: str) -> str:
        """HTML ì½˜í…ì¸ ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤."""
        if not content:
            return ""

        # HTML íƒœê·¸ ì œê±° (ê°„ë‹¨í•œ ë°©ì‹)
        import re

        clean_content = re.sub(r"<[^>]+>", "", content)

        # HTML ì—”í‹°í‹° ì •ë¦¬
        clean_content = clean_content.replace("&nbsp;", " ")
        clean_content = clean_content.replace("&amp;", "&")
        clean_content = clean_content.replace("&lt;", "<")
        clean_content = clean_content.replace("&gt;", ">")
        clean_content = clean_content.replace("&quot;", '"')

        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        clean_content = re.sub(r"\s+", " ", clean_content)

        return clean_content.strip()

    def _parse_published_date(self, entry) -> Optional[datetime]:
        """RSS ì—”íŠ¸ë¦¬ì—ì„œ ë°œí–‰ì¼ì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
        if not hasattr(entry, "published_parsed") or not entry.published_parsed:
            return None

        try:
            parsed_date = entry.published_parsed
            if isinstance(parsed_date, (list, tuple)) and len(parsed_date) >= 6:
                # ì•ˆì „í•œ ë‚ ì§œ ë³€í™˜
                date_parts = []
                for i in range(6):
                    if i < len(parsed_date) and parsed_date[i] is not None:
                        try:
                            date_parts.append(int(parsed_date[i]))
                        except (ValueError, TypeError):
                            date_parts.append(0)
                    else:
                        date_parts.append(0)

                return datetime(*date_parts)
        except Exception as e:
            logger.debug(f"ë°œí–‰ì¼ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None

        return None

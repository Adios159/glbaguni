#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RSS ì„œë¹„ìŠ¤ ëª¨ë“ˆ
RSS í”¼ë“œ ìˆ˜ì§‘ ë° ê´€ë¦¬ë¥¼ ë‹´ë‹¹
"""

import asyncio
import time
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional, Tuple
from urllib.parse import urljoin, urlparse
import xml.etree.ElementTree as ET

import httpx
from bs4 import BeautifulSoup

from ..config import get_settings
from ..utils import get_logger, ContextLogger, log_external_call
from ..utils.exception_handler import ExternalServiceError
from ..utils.validator import validate_url, sanitize_text
from ..models.response_schema import Article, RSSSource

logger = get_logger("services.rss")


class RSSService:
    """
    RSS í”¼ë“œ ìˆ˜ì§‘ ë° ê´€ë¦¬ ì„œë¹„ìŠ¤
    ë¹„ë™ê¸°ì ìœ¼ë¡œ ì—¬ëŸ¬ RSS ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘
    """
    
    def __init__(self):
        """RSS ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self.settings = get_settings()
        self.client = None
        self._stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_articles": 0
        }
        
        logger.info("âœ… RSS ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        self.client = httpx.AsyncClient(
            timeout=httpx.Timeout(self.settings.rss_timeout),
            headers={
                'User-Agent': 'Glbaguni-NewsBot/3.0 (+https://glbaguni.com/bot)',
                'Accept': 'application/rss+xml, application/xml, text/xml, */*',
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                'Cache-Control': 'no-cache'
            },
            follow_redirects=True,
            max_redirects=3
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.client:
            await self.client.aclose()
    
    async def fetch_rss_feeds(
        self,
        rss_urls: List[str],
        max_articles_per_source: Optional[int] = None,
        filter_keywords: Optional[List[str]] = None,
        exclude_keywords: Optional[List[str]] = None
    ) -> Tuple[List[Article], List[Dict[str, Any]]]:
        """
        ì—¬ëŸ¬ RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘
        
        Args:
            rss_urls: RSS URL ëª©ë¡
            max_articles_per_source: ì†ŒìŠ¤ë‹¹ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜
            filter_keywords: í¬í•¨í•  í‚¤ì›Œë“œ
            exclude_keywords: ì œì™¸í•  í‚¤ì›Œë“œ
        
        Returns:
            (ìˆ˜ì§‘ëœ ê¸°ì‚¬ ëª©ë¡, ì†ŒìŠ¤ë³„ í†µê³„)
        """
        
        if max_articles_per_source is None:
            max_articles_per_source = self.settings.max_articles_per_source
        
        with ContextLogger(f"RSS í”¼ë“œ ìˆ˜ì§‘ ({len(rss_urls)}ê°œ ì†ŒìŠ¤)", "rss.fetch_feeds"):
            async with self:  # ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì‚¬ìš©
                # ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ
                semaphore = asyncio.Semaphore(self.settings.max_concurrent_requests)
                
                # ê° RSS ì†ŒìŠ¤ì— ëŒ€í•œ íƒœìŠ¤í¬ ìƒì„±
                tasks = []
                for url in rss_urls:
                    task = self._fetch_single_rss_feed(
                        semaphore, url, max_articles_per_source,
                        filter_keywords, exclude_keywords
                    )
                    tasks.append(task)
                
                # ë³‘ë ¬ ì‹¤í–‰
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # ê²°ê³¼ ì²˜ë¦¬
                all_articles = []
                source_stats = []
                
                for i, result in enumerate(results):
                    url = rss_urls[i]
                    
                    if isinstance(result, Exception):
                        logger.error(f"âŒ RSS ìˆ˜ì§‘ ì‹¤íŒ¨ [{url}]: {result}")
                        source_stats.append({
                            "url": url,
                            "name": self._extract_domain_name(url),
                            "success": False,
                            "error": str(result),
                            "articles_count": 0,
                            "processing_time": 0
                        })
                        self._stats["failed_requests"] += 1
                    else:
                        articles, stats = result
                        all_articles.extend(articles)
                        source_stats.append(stats)
                        self._stats["successful_requests"] += 1
                        self._stats["total_articles"] += len(articles)
                
                self._stats["total_requests"] += len(rss_urls)
                
                logger.info(
                    f"âœ… RSS ìˆ˜ì§‘ ì™„ë£Œ - ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ "
                    f"(ì„±ê³µ: {len([s for s in source_stats if s['success']])}/{len(rss_urls)})"
                )
                
                return all_articles, source_stats
    
    async def _fetch_single_rss_feed(
        self,
        semaphore: asyncio.Semaphore,
        url: str,
        max_articles: int,
        filter_keywords: Optional[List[str]],
        exclude_keywords: Optional[List[str]]
    ) -> Tuple[List[Article], Dict[str, Any]]:
        """
        ë‹¨ì¼ RSS í”¼ë“œ ìˆ˜ì§‘
        
        Args:
            semaphore: ë™ì‹œ ì‹¤í–‰ ì œí•œ
            url: RSS URL
            max_articles: ìµœëŒ€ ê¸°ì‚¬ ìˆ˜
            filter_keywords: í¬í•¨í•  í‚¤ì›Œë“œ
            exclude_keywords: ì œì™¸í•  í‚¤ì›Œë“œ
        
        Returns:
            (ê¸°ì‚¬ ëª©ë¡, í†µê³„ ì •ë³´)
        """
        
        async with semaphore:
            start_time = time.time()
            
            try:
                # URL ê²€ì¦
                validated_url = validate_url(url)
                
                # RSS í”¼ë“œ ë‹¤ìš´ë¡œë“œ
                logger.debug(f"ğŸ“¡ RSS ìš”ì²­ ì‹œì‘: {url}")
                
                response = await self.client.get(validated_url)
                response.raise_for_status()
                
                processing_time = time.time() - start_time
                
                log_external_call(
                    "RSS", urlparse(url).netloc, processing_time, True
                )
                
                # RSS íŒŒì‹±
                articles = await self._parse_rss_content(
                    response.text, url, max_articles, filter_keywords, exclude_keywords
                )
                
                total_time = time.time() - start_time
                
                # í†µê³„ ì •ë³´ ìƒì„±
                stats = {
                    "url": url,
                    "name": self._extract_feed_title(response.text) or self._extract_domain_name(url),
                    "success": True,
                    "articles_count": len(articles),
                    "processing_time": total_time,
                    "response_size": len(response.text),
                    "status_code": response.status_code
                }
                
                logger.debug(f"âœ… RSS ìˆ˜ì§‘ ì„±ê³µ [{url}]: {len(articles)}ê°œ ê¸°ì‚¬ ({total_time:.2f}ì´ˆ)")
                
                return articles, stats
                
            except httpx.HTTPStatusError as e:
                error_msg = f"HTTP {e.response.status_code}: {e.response.text[:200]}"
                logger.warning(f"âš ï¸ RSS HTTP ì˜¤ë¥˜ [{url}]: {error_msg}")
                
                log_external_call(
                    "RSS", urlparse(url).netloc, time.time() - start_time, False
                )
                
                raise ExternalServiceError("RSS", error_msg)
                
            except httpx.TimeoutException:
                error_msg = f"ìš”ì²­ ì‹œê°„ ì´ˆê³¼ ({self.settings.rss_timeout}ì´ˆ)"
                logger.warning(f"â° RSS íƒ€ì„ì•„ì›ƒ [{url}]: {error_msg}")
                
                log_external_call(
                    "RSS", urlparse(url).netloc, time.time() - start_time, False
                )
                
                raise ExternalServiceError("RSS", error_msg)
                
            except Exception as e:
                error_msg = f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}"
                logger.error(f"âŒ RSS ì˜¤ë¥˜ [{url}]: {error_msg}")
                
                log_external_call(
                    "RSS", urlparse(url).netloc, time.time() - start_time, False
                )
                
                raise ExternalServiceError("RSS", error_msg)
    
    async def _parse_rss_content(
        self,
        xml_content: str,
        source_url: str,
        max_articles: int,
        filter_keywords: Optional[List[str]],
        exclude_keywords: Optional[List[str]]
    ) -> List[Article]:
        """
        RSS XML ì½˜í…ì¸  íŒŒì‹±
        
        Args:
            xml_content: RSS XML ë¬¸ìì—´
            source_url: RSS ì†ŒìŠ¤ URL
            max_articles: ìµœëŒ€ ê¸°ì‚¬ ìˆ˜
            filter_keywords: í¬í•¨í•  í‚¤ì›Œë“œ
            exclude_keywords: ì œì™¸í•  í‚¤ì›Œë“œ
        
        Returns:
            íŒŒì‹±ëœ ê¸°ì‚¬ ëª©ë¡
        """
        
        try:
            # XML íŒŒì‹±
            root = ET.fromstring(xml_content)
            
            # RSS 2.0 ë˜ëŠ” Atom í”¼ë“œ ê°ì§€
            if root.tag == 'rss':
                articles = self._parse_rss_20(root, source_url)
            elif root.tag.endswith('feed'):  # Atom
                articles = self._parse_atom_feed(root, source_url)
            else:
                logger.warning(f"âš ï¸ ì§€ì›ë˜ì§€ ì•ŠëŠ” í”¼ë“œ í˜•ì‹: {root.tag}")
                return []
            
            # í‚¤ì›Œë“œ í•„í„°ë§
            if filter_keywords or exclude_keywords:
                articles = self._filter_articles_by_keywords(
                    articles, filter_keywords, exclude_keywords
                )
            
            # ê¸°ì‚¬ ìˆ˜ ì œí•œ
            articles = articles[:max_articles]
            
            logger.debug(f"ğŸ“„ RSS íŒŒì‹± ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬")
            
            return articles
            
        except ET.ParseError as e:
            logger.error(f"âŒ XML íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
            return []
        except Exception as e:
            logger.error(f"âŒ RSS íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
            return []
    
    def _parse_rss_20(self, root: ET.Element, source_url: str) -> List[Article]:
        """RSS 2.0 í˜•ì‹ íŒŒì‹±"""
        
        articles = []
        
        # ì±„ë„ ì •ë³´ ì¶”ì¶œ
        channel = root.find('channel')
        if channel is None:
            return articles
        
        source_name = self._get_text_content(channel.find('title')) or self._extract_domain_name(source_url)
        
        # ê° ì•„ì´í…œ(ê¸°ì‚¬) íŒŒì‹±
        for item in channel.findall('item'):
            try:
                title = self._get_text_content(item.find('title'))
                link = self._get_text_content(item.find('link'))
                description = self._get_text_content(item.find('description'))
                pub_date = self._get_text_content(item.find('pubDate'))
                category = self._get_text_content(item.find('category'))
                
                if not title or not link:
                    continue  # í•„ìˆ˜ í•„ë“œê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                
                # ë‚ ì§œ íŒŒì‹±
                published_at = self._parse_date(pub_date) if pub_date else None
                
                # HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ë¦¬
                clean_description = self._clean_html_content(description) if description else None
                
                # Article ê°ì²´ ìƒì„±
                article = Article(
                    title=sanitize_text(title, 200),
                    content=sanitize_text(clean_description, 2000) if clean_description else None,
                    url=link,
                    source=source_name,
                    published_at=published_at,
                    category=sanitize_text(category, 50) if category else None,
                    language="ko"  # ê¸°ë³¸ê°’, ì‹¤ì œë¡œëŠ” ê°ì§€ ë¡œì§ í•„ìš”
                )
                
                articles.append(article)
                
            except Exception as e:
                logger.warning(f"âš ï¸ ê¸°ì‚¬ íŒŒì‹± ìŠ¤í‚µ: {str(e)}")
                continue
        
        return articles
    
    def _parse_atom_feed(self, root: ET.Element, source_url: str) -> List[Article]:
        """Atom í”¼ë“œ íŒŒì‹±"""
        
        articles = []
        
        # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì²˜ë¦¬
        ns = {'atom': 'http://www.w3.org/2005/Atom'}
        
        # í”¼ë“œ ì œëª© ì¶”ì¶œ
        feed_title_elem = root.find('atom:title', ns)
        source_name = feed_title_elem.text if feed_title_elem is not None else self._extract_domain_name(source_url)
        
        # ê° ì—”íŠ¸ë¦¬(ê¸°ì‚¬) íŒŒì‹±
        for entry in root.findall('atom:entry', ns):
            try:
                title_elem = entry.find('atom:title', ns)
                link_elem = entry.find('atom:link', ns)
                summary_elem = entry.find('atom:summary', ns)
                content_elem = entry.find('atom:content', ns)
                updated_elem = entry.find('atom:updated', ns)
                category_elem = entry.find('atom:category', ns)
                
                title = title_elem.text if title_elem is not None else None
                link = link_elem.get('href') if link_elem is not None else None
                
                # ë‚´ìš© ìš°ì„ ìˆœìœ„: content > summary
                content = None
                if content_elem is not None:
                    content = content_elem.text
                elif summary_elem is not None:
                    content = summary_elem.text
                
                updated = updated_elem.text if updated_elem is not None else None
                category = category_elem.get('term') if category_elem is not None else None
                
                if not title or not link:
                    continue
                
                # ë‚ ì§œ íŒŒì‹±
                published_at = self._parse_iso_date(updated) if updated else None
                
                # HTML ì •ë¦¬
                clean_content = self._clean_html_content(content) if content else None
                
                # Article ê°ì²´ ìƒì„±
                article = Article(
                    title=sanitize_text(title, 200),
                    content=sanitize_text(clean_content, 2000) if clean_content else None,
                    url=link,
                    source=source_name,
                    published_at=published_at,
                    category=sanitize_text(category, 50) if category else None,
                    language="ko"
                )
                
                articles.append(article)
                
            except Exception as e:
                logger.warning(f"âš ï¸ Atom ì—”íŠ¸ë¦¬ íŒŒì‹± ìŠ¤í‚µ: {str(e)}")
                continue
        
        return articles
    
    def _filter_articles_by_keywords(
        self,
        articles: List[Article],
        filter_keywords: Optional[List[str]],
        exclude_keywords: Optional[List[str]]
    ) -> List[Article]:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê¸°ì‚¬ í•„í„°ë§"""
        
        filtered_articles = []
        
        for article in articles:
            # ê²€ìƒ‰ ëŒ€ìƒ í…ìŠ¤íŠ¸ (ì œëª© + ë‚´ìš©)
            search_text = (article.title + " " + (article.content or "")).lower()
            
            # ì œì™¸ í‚¤ì›Œë“œ í™•ì¸
            if exclude_keywords:
                should_exclude = any(
                    keyword.lower() in search_text 
                    for keyword in exclude_keywords
                )
                if should_exclude:
                    continue
            
            # í¬í•¨ í‚¤ì›Œë“œ í™•ì¸
            if filter_keywords:
                should_include = any(
                    keyword.lower() in search_text 
                    for keyword in filter_keywords
                )
                if not should_include:
                    continue
            
            filtered_articles.append(article)
        
        if filter_keywords or exclude_keywords:
            logger.debug(
                f"ğŸ” í‚¤ì›Œë“œ í•„í„°ë§: {len(articles)} â†’ {len(filtered_articles)}ê°œ ê¸°ì‚¬"
            )
        
        return filtered_articles
    
    def _get_text_content(self, element: Optional[ET.Element]) -> Optional[str]:
        """XML ì—˜ë¦¬ë¨¼íŠ¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        if element is None:
            return None
        
        # CDATA ì„¹ì…˜ ì²˜ë¦¬
        text = element.text or ""
        
        # ìì‹ ìš”ì†Œë“¤ì˜ í…ìŠ¤íŠ¸ë„ í¬í•¨
        for child in element:
            if child.text:
                text += child.text
            if child.tail:
                text += child.tail
        
        return text.strip() if text else None
    
    def _clean_html_content(self, html_content: str) -> str:
        """HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not html_content:
            return ""
        
        try:
            # BeautifulSoupë¡œ HTML íŒŒì‹±
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
            for script in soup(["script", "style"]):
                script.decompose()
            
            # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
            text = soup.get_text()
            
            # ê³µë°± ì •ë¦¬
            import re
            text = re.sub(r'\s+', ' ', text)
            
            return text.strip()
            
        except Exception as e:
            logger.warning(f"âš ï¸ HTML ì •ë¦¬ ì‹¤íŒ¨: {str(e)}")
            # ê¸°ë³¸ íƒœê·¸ ì œê±°ë§Œ ìˆ˜í–‰
            import re
            return re.sub(r'<[^>]+>', '', html_content).strip()
    
    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """RSS ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹± (RFC 2822 í˜•ì‹)"""
        if not date_str:
            return None
        
        try:
            from email.utils import parsedate_to_datetime
            return parsedate_to_datetime(date_str)
        except Exception as e:
            logger.warning(f"âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_str} - {str(e)}")
            return None
    
    def _parse_iso_date(self, date_str: str) -> Optional[datetime]:
        """ISO 8601 ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹±"""
        if not date_str:
            return None
        
        try:
            # ë‹¤ì–‘í•œ ISO í˜•ì‹ ì§€ì›
            from dateutil.parser import parse
            return parse(date_str)
        except Exception as e:
            logger.warning(f"âš ï¸ ISO ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_str} - {str(e)}")
            return None
    
    def _extract_feed_title(self, xml_content: str) -> Optional[str]:
        """RSS í”¼ë“œì—ì„œ ì œëª© ì¶”ì¶œ"""
        try:
            root = ET.fromstring(xml_content)
            
            if root.tag == 'rss':
                channel = root.find('channel')
                if channel is not None:
                    title_elem = channel.find('title')
                    if title_elem is not None:
                        return title_elem.text
            elif root.tag.endswith('feed'):
                ns = {'atom': 'http://www.w3.org/2005/Atom'}
                title_elem = root.find('atom:title', ns)
                if title_elem is not None:
                    return title_elem.text
        except Exception:
            pass
        
        return None
    
    def _extract_domain_name(self, url: str) -> str:
        """URLì—ì„œ ë„ë©”ì¸ëª… ì¶”ì¶œ"""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc
            
            # www. ì œê±°
            if domain.startswith('www.'):
                domain = domain[4:]
            
            return domain
        except Exception:
            return "Unknown Source"
    
    async def validate_rss_feed(self, url: str) -> Dict[str, Any]:
        """
        RSS í”¼ë“œ ìœ íš¨ì„± ê²€ì¦
        
        Args:
            url: ê²€ì¦í•  RSS URL
        
        Returns:
            ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        
        result = {
            "is_valid": False,
            "url": url,
            "title": None,
            "article_count": 0,
            "last_updated": None,
            "error": None,
            "response_time": 0
        }
        
        start_time = time.time()
        
        try:
            async with self:
                response = await self.client.get(url)
                response.raise_for_status()
                
                result["response_time"] = time.time() - start_time
                
                # ê¸°ë³¸ íŒŒì‹± ì‹œë„
                articles = await self._parse_rss_content(response.text, url, 5, None, None)
                
                result["is_valid"] = True
                result["title"] = self._extract_feed_title(response.text)
                result["article_count"] = len(articles)
                
                if articles:
                    # ê°€ì¥ ìµœê·¼ ê¸°ì‚¬ì˜ ë‚ ì§œ
                    latest_date = max(
                        (article.published_at for article in articles if article.published_at),
                        default=None
                    )
                    result["last_updated"] = latest_date
                
                logger.info(f"âœ… RSS ê²€ì¦ ì„±ê³µ: {url}")
                
        except Exception as e:
            result["error"] = str(e)
            result["response_time"] = time.time() - start_time
            logger.warning(f"âš ï¸ RSS ê²€ì¦ ì‹¤íŒ¨: {url} - {str(e)}")
        
        return result
    
    def get_stats(self) -> Dict[str, Any]:
        """RSS ì„œë¹„ìŠ¤ í†µê³„ ë°˜í™˜"""
        success_rate = (
            self._stats["successful_requests"] / max(1, self._stats["total_requests"])
        )
        
        return {
            **self._stats,
            "success_rate": success_rate,
            "avg_articles_per_request": (
                self._stats["total_articles"] / max(1, self._stats["successful_requests"])
            )
        }
    
    def reset_stats(self):
        """í†µê³„ ì´ˆê¸°í™”"""
        self._stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_articles": 0
        }
        logger.info("ğŸ“Š RSS ì„œë¹„ìŠ¤ í†µê³„ ì´ˆê¸°í™”")


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    import asyncio
    
    async def test_rss_service():
        print("RSS ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸:")
        
        try:
            service = RSSService()
            
            # í…ŒìŠ¤íŠ¸ RSS URLë“¤
            test_urls = [
                "https://feeds.bbci.co.uk/news/rss.xml",
                "https://rss.cnn.com/rss/edition.rss"
            ]
            
            # RSS í”¼ë“œ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
            articles, stats = await service.fetch_rss_feeds(test_urls, max_articles_per_source=3)
            
            print(f"âœ… ìˆ˜ì§‘ëœ ê¸°ì‚¬: {len(articles)}ê°œ")
            for stat in stats:
                print(f"  - {stat['name']}: {stat['articles_count']}ê°œ ({'ì„±ê³µ' if stat['success'] else 'ì‹¤íŒ¨'})")
            
            # í†µê³„ ì¶œë ¥
            service_stats = service.get_stats()
            print(f"âœ… ì„œë¹„ìŠ¤ í†µê³„: {service_stats}")
            
            # RSS ê²€ì¦ í…ŒìŠ¤íŠ¸
            if test_urls:
                validation = await service.validate_rss_feed(test_urls[0])
                print(f"âœ… RSS ê²€ì¦: {'ìœ íš¨' if validation['is_valid'] else 'ë¬´íš¨'}")
            
        except Exception as e:
            print(f"âŒ RSS ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # ì‹¤ì œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    # asyncio.run(test_rss_service())
    print("RSS ì„œë¹„ìŠ¤ ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
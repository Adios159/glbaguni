#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RSS Service
RSS í”¼ë“œ ì²˜ë¦¬ ì „ìš© ì„œë¹„ìŠ¤
"""

import logging
from datetime import datetime
from typing import List, Optional
from urllib.parse import urlparse

import chardet
import feedparser
import requests
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter

try:
    from ..models import Article
    from .content_extractor import ContentExtractor
except ImportError:
    from content_extractor import ContentExtractor

    from models import Article

logger = logging.getLogger(__name__)


class RSSService:
    """RSS í”¼ë“œ ì²˜ë¦¬ ì„œë¹„ìŠ¤"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Glbaguni RSS Bot)",
                "Accept": "application/rss+xml, application/xml, text/xml, */*",
                "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
            }
        )
        self.session.mount("http://", HTTPAdapter(max_retries=3))
        self.session.mount("https://", HTTPAdapter(max_retries=3))
        self.content_extractor = ContentExtractor()

    def fetch_rss_articles(self, rss_url: str, max_articles: int = 10) -> List[Article]:
        """RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            logger.info(f"ğŸ“¡ RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°: {rss_url}")

            # RSS í”¼ë“œ ìš”ì²­ ë° ì¸ì½”ë”© ì²˜ë¦¬
            response = self.session.get(rss_url, timeout=10)
            response.raise_for_status()

            # ì¸ì½”ë”© ê°ì§€ ë° ë””ì½”ë”©
            detected_encoding = chardet.detect(response.content)
            encoding = (
                detected_encoding.get("encoding", "utf-8")
                if detected_encoding
                else "utf-8"
            )
            logger.info(f"ğŸ” RSS ì¸ì½”ë”©: {encoding}")

            try:
                decoded_content = response.content.decode(encoding, errors="ignore")
            except Exception:
                decoded_content = response.content.decode("utf-8", errors="ignore")

            # RSS íŒŒì‹±
            feed = feedparser.parse(decoded_content)

            return self._process_feed_entries(feed, rss_url, max_articles)

        except Exception as e:
            logger.error(f"âŒ RSS í”¼ë“œ ì²˜ë¦¬ ì‹¤íŒ¨ ({rss_url}): {e}")
            return []

    def _process_feed_entries(
        self, feed, rss_url: str, max_articles: int
    ) -> List[Article]:
        """RSS í”¼ë“œ ì—”íŠ¸ë¦¬ë“¤ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        if not hasattr(feed, "entries") or not feed.entries:
            logger.warning(f"âš ï¸ RSS í”¼ë“œì— ì—”íŠ¸ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {rss_url}")
            return []

        articles = []
        source = urlparse(rss_url).netloc or "unknown"

        for entry in feed.entries[:max_articles]:
            try:
                article = self._create_article_from_entry(entry, source)
                if article:
                    articles.append(article)
            except Exception as e:
                logger.debug(f"ì—”íŠ¸ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        logger.info(f"âœ… RSSì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì™„ë£Œ")
        return articles

    def _create_article_from_entry(self, entry, source: str) -> Optional[Article]:
        """RSS ì—”íŠ¸ë¦¬ì—ì„œ Article ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        if not hasattr(entry, "title") or not hasattr(entry, "link"):
            return None

        title = str(entry.title).strip()
        url = str(entry.link)

        if not title or not url.startswith(("http://", "https://")):
            return None

        # ì½˜í…ì¸  ì¶”ì¶œ
        content = self.content_extractor.extract_content_from_rss_entry(entry)
        if not content or len(content.strip()) < 50:
            return None

        # ë°œí–‰ì¼ íŒŒì‹±
        published_date = self._parse_published_date(entry)

        # Article ê°ì²´ ìƒì„±
        return Article(
            title=title,
            url=url,
            content=content,
            published_date=published_date,
            author=getattr(entry, "author", None),
            source=source,
        )

    def _parse_published_date(self, entry) -> Optional[datetime]:
        """RSS ì—”íŠ¸ë¦¬ì—ì„œ ë°œí–‰ì¼ì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
        if not hasattr(entry, "published_parsed") or not entry.published_parsed:
            return None

        try:
            parsed_date = entry.published_parsed
            if isinstance(parsed_date, (list, tuple)) and len(parsed_date) >= 6:
                # time.struct_timeì„ datetimeìœ¼ë¡œ ë³€í™˜
                date_parts = []
                for i in range(6):
                    if i < len(parsed_date) and parsed_date[i] is not None:
                        try:
                            date_parts.append(int(parsed_date[i]))
                        except (ValueError, TypeError):
                            date_parts.append(0)
                    else:
                        date_parts.append(0)

                return datetime(*date_parts)
        except Exception as e:
            logger.debug(f"ë°œí–‰ì¼ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None

        return None

    def get_default_rss_feeds(self) -> List[str]:
        """ê¸°ë³¸ RSS í”¼ë“œ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return [
            # ì£¼ìš” ë‰´ìŠ¤ ì‚¬ì´íŠ¸
            "https://feeds.feedburner.com/yonhapnews_top",  # ì—°í•©ë‰´ìŠ¤
            "https://rss.sbs.co.kr/news/SectionRssFeed.jsp?sectionId=01",  # SBS
            "http://fs.jtbc.joins.com/RSS/newsflash.xml",  # JTBC
            "https://rss.mk.co.kr/rss/30000001.xml",  # ë§¤ì¼ê²½ì œ
            # ê¸°ìˆ /IT ë‰´ìŠ¤
            "https://feeds.feedburner.com/yonhapnews_it",
            "https://rss.mk.co.kr/rss/50400007.xml",  # ë§¤ê²½ IT
        ]

    def fetch_multiple_rss_feeds(
        self, rss_urls: Optional[List[str]] = None, max_articles_per_feed: int = 5
    ) -> List[Article]:
        """ì—¬ëŸ¬ RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        if not rss_urls:
            rss_urls = self.get_default_rss_feeds()

        all_articles = []

        for rss_url in rss_urls:
            try:
                articles = self.fetch_rss_articles(rss_url, max_articles_per_feed)
                all_articles.extend(articles)
            except Exception as e:
                logger.warning(f"RSS í”¼ë“œ ì²˜ë¦¬ ì‹¤íŒ¨ ({rss_url}): {e}")
                continue

        logger.info(f"ğŸ“Š ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
        return all_articles

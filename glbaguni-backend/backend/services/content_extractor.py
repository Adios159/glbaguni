#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Content Extractor Service
HTML ë° RSS ì½˜í…ì¸  ì¶”ì¶œ ì „ìš© ì„œë¹„ìŠ¤
"""

import logging
from typing import Any, Optional
from urllib.parse import urlparse

import chardet
import requests
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)


class ContentExtractor:
    """HTML ë° RSS ì½˜í…ì¸  ì¶”ì¶œ ì„œë¹„ìŠ¤"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": "Mozilla/5.0 (Glbaguni RSS Bot)"})

    def detect_encoding(self, response: requests.Response) -> str:
        """ì‘ë‹µì˜ ì¸ì½”ë”©ì„ ê°ì§€í•©ë‹ˆë‹¤."""
        # Content-Type í—¤ë”ì—ì„œ charset í™•ì¸
        content_type = response.headers.get("content-type", "").lower()
        if "charset=" in content_type:
            charset = content_type.split("charset=")[1].split(";")[0].strip()
            logger.info(f"ğŸ” [ENCODING] Header charset: {charset}")
            return charset

        # chardetìœ¼ë¡œ ê°ì§€
        detected = chardet.detect(response.content)
        encoding = detected.get("encoding", "utf-8") if detected else "utf-8"
        confidence = detected.get("confidence", 0) if detected else 0

        logger.info(
            f"ğŸ” [ENCODING] Detected: {encoding} (confidence: {confidence:.2f})"
        )

        # í•œêµ­ì–´ ì‚¬ì´íŠ¸ë¥¼ ìœ„í•œ encoding ìš°ì„ ìˆœìœ„
        if confidence < 0.7 and any(
            korean_domain in response.url.lower()
            for korean_domain in [".co.kr", "naver", "daum", "joins", "chosun", "jtbc"]
        ):
            return "utf-8"  # í•œêµ­ ì‚¬ì´íŠ¸ëŠ” ë³´í†µ utf-8

        return encoding

    def extract_content_korean(self, soup: BeautifulSoup) -> str:
        """í•œêµ­ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì— íŠ¹í™”ëœ ì½˜í…ì¸  ì¶”ì¶œ"""
        content_parts = []

        # í•œêµ­ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ë³„ ì…€ë ‰í„° ì •ì˜
        korean_selectors = [
            # SBS
            ".article-text-area",
            ".text_area",
            ".article_txt",
            # JTBC
            ".article_content",
            ".news_content",
            ".content_text",
            # ì—°í•©ë‰´ìŠ¤
            ".story-news-article",
            ".article-txt",
            ".content",
            # ì¡°ì„ ì¼ë³´
            ".par",
            ".article_body",
            ".news_article_body",
            # ì¤‘ì•™ì¼ë³´
            ".article_body",
            ".news_text",
            ".article_content",
            # í•œê²¨ë ˆ
            ".text",
            ".article-text",
            ".content-text",
            # MBC
            ".news_txt",
            ".article_area",
            ".content_area",
            # ì¼ë°˜ì ì¸ ì…€ë ‰í„°
            "article",
            ".article",
            "#article",
            ".post-content",
            ".entry-content",
            ".content",
            ".main-content",
        ]

        for selector in korean_selectors:
            try:
                elements = soup.select(selector)
                if elements:
                    for element in elements:
                        text = element.get_text(strip=True)
                        if text and len(text) > 100:  # ì˜ë¯¸ìˆëŠ” ê¸¸ì´ì˜ í…ìŠ¤íŠ¸ë§Œ
                            content_parts.append(text)
                            logger.info(
                                f"âœ… [EXTRACT] Found content with selector: {selector}"
                            )
                            break
                if content_parts:
                    break
            except Exception as e:
                logger.debug(f"âš ï¸ [EXTRACT] Selector {selector} failed: {e}")
                continue

        return " ".join(content_parts)

    def clean_korean_text(self, text: str) -> str:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì •ë¦¬"""
        if not text:
            return ""

        # ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ ì œê±°
        unwanted_phrases = [
            "ì €ì‘ê¶Œì â“’",
            "ë¬´ë‹¨ì „ì¬ ë° ì¬ë°°í¬ ê¸ˆì§€",
            "ê¸°ì =",
            "íŠ¹íŒŒì› =",
            "= ê¸°ì",
            "ë³¸ ê¸°ì‚¬ëŠ”",
            "ì´ ê¸°ì‚¬ëŠ”",
            "â–²",
            "â–¼",
            "â—†",
            "â—‡",
            "Copyright",
            "All rights reserved",
            "ë‰´ìŠ¤1",
            "ì—°í•©ë‰´ìŠ¤",
            "ë”ë³´ê¸°",
            "ê´€ë ¨ê¸°ì‚¬",
            "â“’ í•œê²½ë‹·ì»´",
            "í•œêµ­ê²½ì œ",
            "ë§¤ì¼ê²½ì œ",
            "í˜ì´ìŠ¤ë¶",
            "íŠ¸ìœ„í„°",
            "ì¹´ì¹´ì˜¤í†¡",
            "ë„¤ì´ë²„",
            "URLë³µì‚¬",
        ]

        for phrase in unwanted_phrases:
            text = text.replace(phrase, "")

        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        import re

        text = re.sub(r"\s+", " ", text)
        text = re.sub(r"\n+", "\n", text)

        return text.strip()

    def extract_content_fallback(self, soup: BeautifulSoup) -> str:
        """ì¼ë°˜ì ì¸ ì½˜í…ì¸  ì¶”ì¶œ ë°©ë²•"""
        content_tags = ["article", "main", ".content", ".post", ".entry"]

        for tag in content_tags:
            try:
                if tag.startswith("."):
                    elements = soup.select(tag)
                else:
                    elements = soup.find_all(tag)

                if elements:
                    content = elements[0].get_text(strip=True)
                    if len(content) > 100:
                        return content
            except Exception:
                continue

        # ë§ˆì§€ë§‰ ìˆ˜ë‹¨: body íƒœê·¸ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        body = soup.find("body")
        if body:
            return body.get_text(strip=True)

        return soup.get_text(strip=True)

    def extract_content_from_rss_entry(self, entry: Any) -> str:
        """RSS ì—”íŠ¸ë¦¬ì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ"""
        content = ""

        # RSS í•„ë“œ ìš°ì„ ìˆœìœ„ ìˆœì„œë¡œ í™•ì¸
        content_fields = [
            "content",  # Atom í‘œì¤€
            "summary",  # RSS í‘œì¤€
            "description",  # RSS í‘œì¤€
            "summary_detail",  # feedparser í™•ì¥
            "content_detail",  # feedparser í™•ì¥
        ]

        for field in content_fields:
            if hasattr(entry, field):
                field_content = getattr(entry, field)

                if isinstance(field_content, list) and field_content:
                    # content, summary_detail ë“±ì€ ë¦¬ìŠ¤íŠ¸
                    field_content = field_content[0]

                if isinstance(field_content, dict):
                    # detail ê°ì²´ì—ì„œ value ì¶”ì¶œ
                    field_content = field_content.get("value", "") or field_content.get(
                        "content", ""
                    )

                if isinstance(field_content, str) and field_content.strip():
                    content = field_content
                    break

        if not content:
            return ""

        return self.clean_rss_content(content)

    def clean_rss_content(self, content: str) -> str:
        """RSS ì½˜í…ì¸  ì •ë¦¬"""
        if not content:
            return ""

        # HTML íƒœê·¸ ì œê±°
        try:
            from bs4 import BeautifulSoup

            soup = BeautifulSoup(content, "html.parser")
            content = soup.get_text()
        except:
            pass

        # ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ ì œê±°
        unwanted = [
            "The post",
            "appeared first on",
            "Continue reading",
            "[Read more...]",
            "Read more",
            "ë”ë³´ê¸°",
            "ì „ì²´ë³´ê¸°",
            "&nbsp;",
            "&amp;",
            "&lt;",
            "&gt;",
            "&quot;",
        ]

        for phrase in unwanted:
            content = content.replace(phrase, "")

        return content.strip()

    def extract_title(self, soup: BeautifulSoup) -> str:
        """í˜ì´ì§€ ì œëª© ì¶”ì¶œ"""
        # ì œëª© íƒœê·¸ ìš°ì„ ìˆœìœ„
        title_selectors = [
            "h1.title",
            "h1.headline",
            ".article-title h1",
            ".news-title",
            ".post-title",
            "h1",
            "title",
        ]

        for selector in title_selectors:
            try:
                element = soup.select_one(selector)
                if element:
                    title = element.get_text(strip=True)
                    if title and len(title) > 5:
                        return title
            except Exception:
                continue

        # ê¸°ë³¸ê°’: HTML title íƒœê·¸
        title_tag = soup.find("title")
        if title_tag:
            return title_tag.get_text(strip=True)

        return "ì œëª© ì—†ìŒ"

    def extract_content(self, soup: BeautifulSoup) -> str:
        """í†µí•© ì½˜í…ì¸  ì¶”ì¶œ"""
        # 1ë‹¨ê³„: í•œêµ­ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ íŠ¹í™” ì¶”ì¶œ
        content = self.extract_content_korean(soup)

        if content and len(content.strip()) > 100:
            return self.clean_korean_text(content)

        # 2ë‹¨ê³„: ì¼ë°˜ì ì¸ ë°©ë²•ìœ¼ë¡œ ì¶”ì¶œ
        content = self.extract_content_fallback(soup)

        if content:
            return self.clean_korean_text(content)

        return "ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨"

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Fetch Router
RSS í”¼ë“œ ë° ë‰´ìŠ¤ ìˆ˜ì§‘ ê´€ë ¨ ì—”ë“œí¬ì¸íŠ¸
"""

import uuid
from datetime import datetime
from typing import List, Optional

from fastapi import APIRouter, BackgroundTasks, HTTPException, Request
from pydantic import BaseModel, HttpUrl

# ì˜ì¡´ì„± ì„í¬íŠ¸
try:
    from services.content_extractor import ContentExtractor
    from services.rss_service import RSSService
    from utils.validator import validate_user_input, validate_url
except ImportError:
    try:
        from backend.services.content_extractor import ContentExtractor
        from backend.services.rss_service import RSSService
        from backend.utils.validator import validate_user_input, validate_url
    except ImportError:
        # ê¸°ë³¸ê°’ìœ¼ë¡œ None ë˜ëŠ” Mock ê°ì²´ ì‚¬ìš©
        ContentExtractor = None
        RSSService = None
        def validate_user_input(text: str, max_length: int = 5000) -> str:
            return text
        def validate_url(url: str) -> str:
            return url

import logging

logger = logging.getLogger(__name__)

# ë¼ìš°í„° ìƒì„±
router = APIRouter(prefix="/fetch", tags=["fetch"])


# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
class RSSFetchRequest(BaseModel):
    """RSS í”¼ë“œ ìš”ì²­ ëª¨ë¸"""

    rss_urls: List[HttpUrl]
    max_articles: int = 10
    user_id: Optional[str] = None


class ArticleFetchRequest(BaseModel):
    """ê°œë³„ ê¸°ì‚¬ ìš”ì²­ ëª¨ë¸"""

    url: HttpUrl
    user_id: Optional[str] = None


class FetchResponse(BaseModel):
    """ìˆ˜ì§‘ ì‘ë‹µ ëª¨ë¸"""

    success: bool
    message: str
    articles_count: int
    articles: List[dict]
    request_id: str
    processed_at: str


# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ (ì•ˆì „í•œ ì´ˆê¸°í™”)
try:
    rss_service = RSSService() if RSSService else None
    content_extractor = ContentExtractor() if ContentExtractor else None
except Exception:
    rss_service = None
    content_extractor = None


@router.post("/rss", response_model=FetchResponse)
async def fetch_rss_feeds(request: RSSFetchRequest, background_tasks: BackgroundTasks):
    """
    RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ë“¤ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

    - **rss_urls**: RSS í”¼ë“œ URL ëª©ë¡
    - **max_articles**: í”¼ë“œë‹¹ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸ê°’: 10)
    - **user_id**: ì‚¬ìš©ì ID (ì„ íƒì‚¬í•­)
    """
    request_id = str(uuid.uuid4())[:8]

    try:
        # 1. ìš”ì²­ ìˆ˜ì‹ 
        logger.info(f"ğŸ“¥ [RSSìš”ì²­] ì‚¬ìš©ì ì…ë ¥ ìˆ˜ì‹  ì™„ë£Œ - ID: {request_id}")
        logger.info(f"ğŸ“¡ [{request_id}] RSS í”¼ë“œ ìˆ˜ì§‘ ìš”ì²­: {len(request.rss_urls)}ê°œ í”¼ë“œ")

        # 2. ì…ë ¥ ê²€ì¦ ì‹œì‘
        logger.info(f"ğŸ” [ê²€ì¦] RSS URL ê²€ì¦ ì‹œì‘ - ID: {request_id}")
        
        # URL ê²€ì¦
        validated_urls = []
        for url in request.rss_urls:
            try:
                validated_url = validate_url(str(url))
                validated_urls.append(validated_url)
            except Exception as e:
                logger.warning(f"âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ URL ìŠ¤í‚µ: {url} - {e}")
                continue

        if not validated_urls:
            raise HTTPException(status_code=400, detail="ìœ íš¨í•œ RSS URLì´ ì—†ìŠµë‹ˆë‹¤.")

        # 2. ì…ë ¥ ê²€ì¦ ì™„ë£Œ
        logger.info(f"ğŸ” [ê²€ì¦] RSS URL ê²€ì¦ ì™„ë£Œ - ID: {request_id}, ìœ íš¨í•œ URL: {len(validated_urls)}ê°œ")

        # 3. ì²˜ë¦¬ ì‹œì‘
        logger.info(f"âš™ï¸ [ì²˜ë¦¬] RSS í”¼ë“œ ìˆ˜ì§‘ ì‹¤í–‰ ì‹œì‘ - ID: {request_id}")

        # RSS í”¼ë“œ ìˆ˜ì§‘
        all_articles = []
        if rss_service:
            for rss_url in validated_urls:
                try:
                    # RSS ì„œë¹„ìŠ¤ê°€ ìˆë‹¤ë©´ ê°„ë‹¨í•œ ë”ë¯¸ ë°ì´í„° ë°˜í™˜
                    # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì ì ˆí•œ ë©”ì„œë“œ í˜¸ì¶œ
                    all_articles.append({
                        'title': f'Sample Article from {rss_url}',
                        'url': rss_url,
                        'content': 'Sample content',
                        'source': 'RSS',
                        'published_at': None
                    })
                except Exception as e:
                    logger.warning(f"âš ï¸ RSS í”¼ë“œ ìˆ˜ì§‘ ì‹¤íŒ¨ ({rss_url}): {e}")
                    continue
        else:
            logger.warning("RSS ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # 4. ì²˜ë¦¬ ì™„ë£Œ
        logger.info(f"âœ… [ì™„ë£Œ] RSS í”¼ë“œ ìˆ˜ì§‘ ì™„ë£Œ - ID: {request_id}, ìˆ˜ì§‘ëœ ê¸°ì‚¬: {len(all_articles)}ê°œ")

        # ì‘ë‹µ ìƒì„±
        articles_data = []
        for article in all_articles:
            published_at = article.get('published_at') if isinstance(article, dict) else getattr(article, 'published_at', None)
            articles_data.append(
                {
                    "title": article.get('title', 'Untitled') if isinstance(article, dict) else getattr(article, 'title', 'Untitled'),
                    "url": str(article.get('url', '') if isinstance(article, dict) else getattr(article, 'url', '')),
                    "content": (
                        (article.get('content', '') if isinstance(article, dict) else getattr(article, 'content', ''))[:500] + "..."
                        if len(article.get('content', '') if isinstance(article, dict) else getattr(article, 'content', '')) > 500
                        else (article.get('content', '') if isinstance(article, dict) else getattr(article, 'content', ''))
                    ),
                    "source": article.get('source', 'Unknown') if isinstance(article, dict) else getattr(article, 'source', 'Unknown'),
                    "published_date": (
                        published_at.isoformat()
                        if published_at and hasattr(published_at, 'isoformat')
                        else None
                    ),
                }
            )

        # 5. ì‘ë‹µ ì „ì†¡
        logger.info(f"ğŸ“¤ [ì‘ë‹µ] í´ë¼ì´ì–¸íŠ¸ì— ì‘ë‹µ ì „ì†¡ - ID: {request_id}")

        return FetchResponse(
            success=True,
            message=f"{len(all_articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.",
            articles_count=len(all_articles),
            articles=articles_data,
            request_id=request_id,
            processed_at=datetime.now().isoformat(),
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [ì˜¤ë¥˜] RSS í”¼ë“œ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - ID: {request_id}: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"RSS í”¼ë“œ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.post("/article", response_model=FetchResponse)
async def fetch_single_article(request: ArticleFetchRequest):
    """
    ê°œë³„ ê¸°ì‚¬ URLì—ì„œ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

    - **url**: ê¸°ì‚¬ URL
    - **user_id**: ì‚¬ìš©ì ID (ì„ íƒì‚¬í•­)
    """
    request_id = str(uuid.uuid4())[:8]

    try:
        # 1. ìš”ì²­ ìˆ˜ì‹ 
        logger.info(f"ğŸ“¥ [ê¸°ì‚¬ìš”ì²­] ì‚¬ìš©ì ì…ë ¥ ìˆ˜ì‹  ì™„ë£Œ - ID: {request_id}")
        logger.info(f"ğŸ“„ [{request_id}] ê°œë³„ ê¸°ì‚¬ ìˆ˜ì§‘: {request.url}")

        # 2. ì…ë ¥ ê²€ì¦ ì‹œì‘
        logger.info(f"ğŸ” [ê²€ì¦] ê¸°ì‚¬ URL ê²€ì¦ ì‹œì‘ - ID: {request_id}")
        
        # URL ê²€ì¦
        validated_url = validate_url(str(request.url))
        
        # 2. ì…ë ¥ ê²€ì¦ ì™„ë£Œ
        logger.info(f"ğŸ” [ê²€ì¦] ê¸°ì‚¬ URL ê²€ì¦ ì™„ë£Œ - ID: {request_id}")

        # 3. ì²˜ë¦¬ ì‹œì‘
        logger.info(f"âš™ï¸ [ì²˜ë¦¬] ê¸°ì‚¬ ì½˜í…ì¸  ì¶”ì¶œ ì‹œì‘ - ID: {request_id}")

        # ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ
        import requests
        from bs4 import BeautifulSoup
        from urllib.parse import urlparse

        try:
            response = requests.get(validated_url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")
            
            # ê¸°ë³¸ì ì¸ ì½˜í…ì¸  ì¶”ì¶œ
            title = soup.find('title')
            title_text = title.get_text().strip() if title else "Untitled"
            
            # ë³¸ë¬¸ ì¶”ì¶œ (ê°„ë‹¨í•œ êµ¬í˜„)
            content_tags = soup.find_all(['p', 'div', 'article'])
            content = " ".join([tag.get_text().strip() for tag in content_tags])

            if not content or len(content.strip()) < 50:
                raise HTTPException(
                    status_code=400, detail="ê¸°ì‚¬ ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )

            # 4. ì²˜ë¦¬ ì™„ë£Œ
            logger.info(f"âœ… [ì™„ë£Œ] ê¸°ì‚¬ ì½˜í…ì¸  ì¶”ì¶œ ì™„ë£Œ - ID: {request_id}")

            article_data = {
                "title": title_text,
                "url": validated_url,
                "content": content[:1000] + "..." if len(content) > 1000 else content,
                "source": urlparse(validated_url).netloc,
                "published_date": None,
            }

            # 5. ì‘ë‹µ ì „ì†¡
            logger.info(f"ğŸ“¤ [ì‘ë‹µ] í´ë¼ì´ì–¸íŠ¸ì— ì‘ë‹µ ì „ì†¡ - ID: {request_id}")

            return FetchResponse(
                success=True,
                message="ê¸°ì‚¬ë¥¼ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.",
                articles_count=1,
                articles=[article_data],
                request_id=request_id,
                processed_at=datetime.now().isoformat(),
            )

        except requests.RequestException as e:
            raise HTTPException(
                status_code=400, detail=f"ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}"
            )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [ì˜¤ë¥˜] ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - ID: {request_id}: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get("/default-feeds")
async def get_default_rss_feeds():
    """ê¸°ë³¸ RSS í”¼ë“œ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        default_feeds = [
            {"name": "ì—°í•©ë‰´ìŠ¤", "url": "https://www.yonhapnews.co.kr/rss/news.xml", "category": "general"},
            {"name": "KBS", "url": "http://world.kbs.co.kr/rss/rss_news.htm", "category": "broadcast"},
            {"name": "MBC", "url": "https://imnews.imbc.com/rss/news/news_00.xml", "category": "broadcast"},
        ]

        return {
            "success": True,
            "feeds": default_feeds,
            "count": len(default_feeds),
            "timestamp": datetime.now().isoformat(),
        }

    except Exception as e:
        logger.error(f"ê¸°ë³¸ RSS í”¼ë“œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500, detail="ê¸°ë³¸ RSS í”¼ë“œ ëª©ë¡ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        )


@router.get("/health")
async def fetch_health_check():
    """fetch ë¼ìš°í„° í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "router": "fetch",
        "timestamp": datetime.now().isoformat(),
        "rss_service_available": rss_service is not None,
        "content_extractor_available": content_extractor is not None,
    }

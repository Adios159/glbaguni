#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë¹„ë™ê¸° ê¸°ì‚¬ ìˆ˜ì§‘ ëª¨ë“ˆ v3.0.0
ì™„ì „í•œ async/await íŒ¨í„´ìœ¼ë¡œ ë¦¬íŒ©í† ë§
"""

import asyncio
import logging
import time
from typing import List, Optional, Dict, Any
from datetime import datetime
import re
import uuid

import httpx
import feedparser
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

try:
    from .models import Article
    from .config import settings
except ImportError:
    from models import Article
    from config import settings

logger = logging.getLogger("glbaguni.fetcher")

class AsyncArticleFetcher:
    """ì™„ì „ ë¹„ë™ê¸° ê¸°ì‚¬ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        self.session: Optional[httpx.AsyncClient] = None
        self.timeout = httpx.Timeout(connect=10.0, read=30.0)
        self.limits = httpx.Limits(max_connections=10, max_keepalive_connections=5)
        
        # í—¤ë” ì„¤ì •
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Glbaguni RSS Bot v3.0)',
            'Accept': 'application/rss+xml, application/xml, text/xml, text/html, */*',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate'
        }
    
    async def __aenter__(self):
        """async context manager ì§„ì…"""
        self.session = httpx.AsyncClient(
            timeout=self.timeout,
            limits=self.limits,
            headers=self.headers,
            follow_redirects=True
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """async context manager ì¢…ë£Œ"""
        if self.session:
            await self.session.aclose()
    
    async def fetch_rss_articles(self, rss_url: str, max_articles: int = 10) -> List[Article]:
        """RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ (ì™„ì „ ë¹„ë™ê¸°)"""
        req_id = str(uuid.uuid4())[:8]
        logger.info(f"ğŸ“¡ [{req_id}] RSS ìˆ˜ì§‘ ì‹œì‘: {rss_url}")
        
        try:
            if not self.session:
                raise Exception("HTTP ì„¸ì…˜ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            # RSS ì½˜í…ì¸  ë‹¤ìš´ë¡œë“œ
            response = await self.session.get(rss_url, timeout=30.0)
            response.raise_for_status()
            
            # ì¸ì½”ë”© ê°ì§€ ë° ì²˜ë¦¬
            content = response.content
            if response.encoding:
                try:
                    decoded_content = content.decode(response.encoding, errors='ignore')
                except:
                    decoded_content = content.decode('utf-8', errors='ignore')
            else:
                decoded_content = content.decode('utf-8', errors='ignore')
            
            logger.info(f"ğŸ“¥ [{req_id}] RSS ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(decoded_content)}ì")
            
            # feedparserë¡œ íŒŒì‹± (ë™ê¸° í•¨ìˆ˜ë¥¼ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)
            feed = await asyncio.to_thread(feedparser.parse, decoded_content)
            
            if not hasattr(feed, 'entries') or not feed.entries:
                logger.warning(f"âš ï¸ [{req_id}] RSS ì—”íŠ¸ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            logger.info(f"ğŸ“‹ [{req_id}] {len(feed.entries)}ê°œ ì—”íŠ¸ë¦¬ ë°œê²¬")
            
            # ë³‘ë ¬ë¡œ ê¸°ì‚¬ ì²˜ë¦¬
            tasks = []
            for entry in feed.entries[:max_articles]:
                task = self._process_rss_entry(entry, rss_url, req_id)
                tasks.append(task)
            
            # ëª¨ë“  ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ì„±ê³µí•œ ê²°ê³¼ë§Œ ìˆ˜ì§‘
            articles = []
            for result in results:
                if isinstance(result, Article):
                    articles.append(result)
                elif isinstance(result, Exception):
                    logger.error(f"âŒ [{req_id}] ê¸°ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨: {result}")
            
            logger.info(f"âœ… [{req_id}] RSS ìˆ˜ì§‘ ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬")
            return articles
            
        except Exception as e:
            logger.error(f"ğŸ’¥ [{req_id}] RSS ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return []
    
    async def _process_rss_entry(self, entry: Any, source_url: str, req_id: str) -> Optional[Article]:
        """RSS ì—”íŠ¸ë¦¬ ì²˜ë¦¬ (ë¹„ë™ê¸°)"""
        try:
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            if not hasattr(entry, 'title') or not hasattr(entry, 'link'):
                return None
            
            title = str(entry.title).strip()
            url = str(entry.link).strip()
            
            if not title or not url:
                return None
            
            # URL ìœ íš¨ì„± ê²€ì‚¬
            if not url.startswith(('http://', 'https://')):
                return None
            
            # RSSì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ
            content = self._extract_rss_content(entry)
            
            # ì½˜í…ì¸ ê°€ ë¶€ì¡±í•˜ë©´ ì „ë¬¸ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸° ì‹œë„
            if len(content.strip()) < 100:
                try:
                    full_content = await self._fetch_full_article_content(url)
                    if full_content and len(full_content) > len(content):
                        content = full_content
                except Exception as e:
                    logger.warning(f"âš ï¸ [{req_id}] ì „ë¬¸ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            
            # ìµœì†Œ ì½˜í…ì¸  ê¸¸ì´ í™•ì¸
            if len(content.strip()) < 50:
                return None
            
            # ê²Œì‹œì¼ íŒŒì‹±
            published_date = None
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                try:
                    time_struct = entry.published_parsed[:6]
                    published_date = datetime(*time_struct)
                except:
                    pass
            
            # Article ê°ì²´ ìƒì„±
            return Article(
                title=title,
                url=url,
                content=content,
                published_date=published_date,
                author=getattr(entry, 'author', None),
                source=urlparse(source_url).netloc or 'unknown'
            )
            
        except Exception as e:
            logger.error(f"âŒ [{req_id}] RSS ì—”íŠ¸ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_rss_content(self, entry: Any) -> str:
        """RSS ì—”íŠ¸ë¦¬ì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ"""
        content_candidates = []
        
        # ë‹¤ì–‘í•œ ì½˜í…ì¸  í•„ë“œ ì‹œë„
        for field in ['content', 'summary', 'description']:
            if hasattr(entry, field):
                field_value = getattr(entry, field)
                if isinstance(field_value, list) and field_value:
                    field_value = field_value[0]
                if hasattr(field_value, 'value'):
                    content_candidates.append(field_value.value)
                elif isinstance(field_value, str):
                    content_candidates.append(field_value)
        
        # ê°€ì¥ ê¸´ ì½˜í…ì¸  ì„ íƒ
        if content_candidates:
            content = max(content_candidates, key=len)
            return self._clean_html_content(content)
        
        return ""
    
    def _clean_html_content(self, content: str) -> str:
        """HTML ì½˜í…ì¸  ì •í™”"""
        if not content:
            return ""
        
        try:
            # BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±
            soup = BeautifulSoup(content, 'html.parser')
            
            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
            for tag in soup.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                tag.decompose()
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = soup.get_text(separator=' ', strip=True)
            
            # ì—°ì†ëœ ê³µë°± ì •ë¦¬
            text = re.sub(r'\s+', ' ', text)
            text = re.sub(r'\n\s*\n', '\n', text)
            
            return text.strip()
            
        except Exception as e:
            logger.warning(f"HTML ì •í™” ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ì ì¸ HTML íƒœê·¸ë§Œ ì œê±°
            content = re.sub(r'<[^>]+>', '', content)
            content = re.sub(r'\s+', ' ', content)
            return content.strip()
    
    async def _fetch_full_article_content(self, url: str) -> Optional[str]:
        """ì „ë¬¸ ê¸°ì‚¬ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸° (ë¹„ë™ê¸°)"""
        if not self.session:
            return None
        
        try:
            response = await self.session.get(url, timeout=20.0)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„
            content_selectors = [
                'article',
                '.article-content',
                '.content',
                '.post-content',
                '#content',
                '.entry-content',
                'main'
            ]
            
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content = elements[0].get_text(separator=' ', strip=True)
                    if len(content) > 200:  # ì¶©ë¶„í•œ ê¸¸ì´ì˜ ì½˜í…ì¸ ë§Œ
                        return self._clean_html_content(content)
            
            return None
            
        except Exception as e:
            logger.warning(f"ì „ë¬¸ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ({url}): {e}")
            return None
    
    async def fetch_single_article(self, url: str) -> Optional[Article]:
        """ë‹¨ì¼ ê¸°ì‚¬ URLì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ (ë¹„ë™ê¸°)"""
        req_id = str(uuid.uuid4())[:8]
        logger.info(f"ğŸ“° [{req_id}] ë‹¨ì¼ ê¸°ì‚¬ ìˆ˜ì§‘: {url}")
        
        try:
            if not self.session:
                raise Exception("HTTP ì„¸ì…˜ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            response = await self.session.get(url, timeout=30.0)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ
            title = self._extract_title(soup)
            if not title:
                return None
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content = await self._extract_article_content(soup)
            if not content or len(content) < 100:
                return None
            
            logger.info(f"âœ… [{req_id}] ë‹¨ì¼ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ: {title[:50]}...")
            
            return Article(
                title=title,
                url=url,
                content=content,
                published_date=None,
                author=None,
                source=urlparse(url).netloc or 'unknown'
            )
            
        except Exception as e:
            logger.error(f"ğŸ’¥ [{req_id}] ë‹¨ì¼ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_title(self, soup: BeautifulSoup) -> Optional[str]:
        """ê¸°ì‚¬ ì œëª© ì¶”ì¶œ"""
        title_selectors = [
            'h1.title',
            'h1.article-title',
            'h1#title',
            '.article-header h1',
            'h1',
            'title'
        ]
        
        for selector in title_selectors:
            element = soup.select_one(selector)
            if element:
                title = element.get_text(strip=True)
                if title and len(title) > 10:
                    return title
        
        return None
    
    async def _extract_article_content(self, soup: BeautifulSoup) -> Optional[str]:
        """ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for tag in soup.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside', 'ads']):
            tag.decompose()
        
        # ë³¸ë¬¸ ì¶”ì¶œ ì‹œë„
        content_selectors = [
            'article',
            '.article-content',
            '.content',
            '.post-content',
            '#content',
            '.entry-content',
            '.article-body',
            'main'
        ]
        
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                content = elements[0].get_text(separator=' ', strip=True)
                if len(content) > 200:
                    return self._clean_html_content(content)
        
        # ê¸°ë³¸ ë³¸ë¬¸ ì¶”ì¶œ
        paragraphs = soup.find_all('p')
        if paragraphs:
            content = ' '.join([p.get_text(strip=True) for p in paragraphs])
            if len(content) > 200:
                return content
        
        return None
    
    async def fetch_multiple_sources(
        self,
        rss_urls: Optional[List[str]] = None,
        article_urls: Optional[List[str]] = None,
        max_articles: int = 10
    ) -> List[Article]:
        """ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ (ì™„ì „ ë¹„ë™ê¸°)"""
        req_id = str(uuid.uuid4())[:8]
        start_time = time.time()
        
        logger.info(f"ğŸ¯ [{req_id}] ë‹¤ì¤‘ ì†ŒìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
        logger.info(f"ğŸ“Š [{req_id}] RSS: {len(rss_urls or [])}, ê¸°ì‚¬: {len(article_urls or [])}")
        
        all_articles = []
        tasks = []
        
        try:
            async with self:  # context manager ì‚¬ìš©
                # RSS URLë“¤ ì²˜ë¦¬
                if rss_urls:
                    for rss_url in rss_urls:
                        task = self.fetch_rss_articles(rss_url, max_articles // len(rss_urls) + 1)
                        tasks.append(task)
                
                # ê°œë³„ ê¸°ì‚¬ URLë“¤ ì²˜ë¦¬
                if article_urls:
                    for article_url in article_urls:
                        task = self.fetch_single_article(article_url)
                        tasks.append(task)
                
                # ëª¨ë“  ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
                if tasks:
                    results = await asyncio.gather(*tasks, return_exceptions=True)
                    
                    for result in results:
                        if isinstance(result, list):
                            all_articles.extend(result)
                        elif isinstance(result, Article):
                            all_articles.append(result)
                        elif isinstance(result, Exception):
                            logger.error(f"âŒ [{req_id}] ì‘ì—… ì‹¤íŒ¨: {result}")
            
            # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
            seen_urls = set()
            unique_articles = []
            for article in all_articles:
                if str(article.url) not in seen_urls:
                    seen_urls.add(str(article.url))
                    unique_articles.append(article)
            
            # ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ ì œí•œ
            final_articles = unique_articles[:max_articles]
            
            elapsed = time.time() - start_time
            logger.info(f"ğŸ‰ [{req_id}] ë‹¤ì¤‘ ì†ŒìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(final_articles)}ê°œ ê¸°ì‚¬ ({elapsed:.2f}ì´ˆ)")
            
            return final_articles
            
        except Exception as e:
            logger.error(f"ğŸ’¥ [{req_id}] ë‹¤ì¤‘ ì†ŒìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

# í¸ì˜ í•¨ìˆ˜ë“¤
async def fetch_rss_async(rss_url: str, max_articles: int = 10) -> List[Article]:
    """RSS í”¼ë“œ ë¹„ë™ê¸° ìˆ˜ì§‘ (í¸ì˜ í•¨ìˆ˜)"""
    async with AsyncArticleFetcher() as fetcher:
        return await fetcher.fetch_rss_articles(rss_url, max_articles)

async def fetch_article_async(url: str) -> Optional[Article]:
    """ë‹¨ì¼ ê¸°ì‚¬ ë¹„ë™ê¸° ìˆ˜ì§‘ (í¸ì˜ í•¨ìˆ˜)"""
    async with AsyncArticleFetcher() as fetcher:
        return await fetcher.fetch_single_article(url)
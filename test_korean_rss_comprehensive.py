#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sys
import requests
import feedparser
import chardet
from glbaguni.backend.fetcher import ArticleFetcher
from glbaguni.backend.summarizer import ArticleSummarizer

def test_korean_rss_comprehensive():
    """Test Korean RSS feeds with encoding fixes and comprehensive diagnostics."""
    
    # Target RSS feeds for testing
    rss_feeds = [
        "https://news-ex.jtbc.co.kr/v1/get/rss/newsflesh",
        "https://www.hani.co.kr/rss/",
        "https://imnews.imbc.com/rss/google_news/narrativeNews.rss"
    ]
    
    print("üî¨ KOREAN RSS COMPREHENSIVE TESTING")
    print("=" * 80)
    print()
    
    fetcher = ArticleFetcher()
    summarizer = ArticleSummarizer()
    
    all_results = {}
    
    for i, feed_url in enumerate(rss_feeds, 1):
        print(f"üì∞ TESTING FEED {i}/3: {feed_url}")
        print("=" * 80)
        
        # Step 1: Manual RSS parsing with encoding detection
        print("üîç STEP 1: Manual RSS Parsing with Encoding Detection")
        print("-" * 60)
        
        try:
            # Fetch with proper headers
            response = requests.get(feed_url, headers={
                'User-Agent': 'Mozilla/5.0 (Glbaguni RSS Bot)',
                'Accept': 'application/rss+xml, application/xml, text/xml, */*',
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8'
            }, timeout=15)
            
            print(f"‚úÖ HTTP Status: {response.status_code}")
            print(f"üìÑ Content-Type: {response.headers.get('Content-Type', 'Unknown')}")
            print(f"üìä Content Length: {len(response.content)} bytes")
            
            # Detect encoding
            detected = chardet.detect(response.content)
            encoding = detected.get('encoding', 'utf-8') if detected else 'utf-8'
            confidence = detected.get('confidence', 0) if detected else 0
            
            print(f"üîç Detected Encoding: {encoding} (confidence: {confidence:.2f})")
            
            # Decode content
            try:
                decoded_content = response.content.decode(encoding, errors='ignore')
                print(f"‚úÖ Successfully decoded {len(decoded_content)} characters")
            except Exception as e:
                print(f"‚ö†Ô∏è  Decode error, falling back to utf-8: {e}")
                decoded_content = response.content.decode('utf-8', errors='ignore')
            
            # Parse with feedparser
            feed = feedparser.parse(decoded_content)
            
            print(f"üö® Bozo Flag: {feed.bozo} ({'Parsing issues detected' if feed.bozo else 'No issues'})")
            if feed.bozo and hasattr(feed, 'bozo_exception'):
                print(f"‚ö†Ô∏è  Bozo Exception: {feed.bozo_exception}")
            
            print(f"üìÑ Total Entries: {len(feed.entries)}")
            
            if hasattr(feed, 'feed'):
                print(f"üì∞ Feed Title: {getattr(feed.feed, 'title', 'Unknown')}")
                print(f"üîó Feed Link: {getattr(feed.feed, 'link', 'Unknown')}")
            
            # Show first 3 entries
            if len(feed.entries) > 0:
                print(f"üìã First 3 Entries:")
                for j, entry in enumerate(feed.entries[:3]):
                    print(f"  {j+1}. Title: {getattr(entry, 'title', 'No title')}")
                    print(f"     Link: {getattr(entry, 'link', 'No link')}")
                    print()
            else:
                print("‚ùå No entries found")
                
        except Exception as e:
            print(f"‚ùå Error in manual parsing: {e}")
            feed = feedparser.parse(feed_url)  # Fallback
        
        print()
        
        # Step 2: Article fetching with our enhanced fetcher
        print("üîß STEP 2: Enhanced Article Fetching")
        print("-" * 60)
        
        articles = fetcher.fetch_rss_articles(feed_url, max_articles=5)
        
        print(f"üìä Articles Fetched: {len(articles)}")
        
        if articles:
            print("üìã Article Details:")
            for j, article in enumerate(articles, 1):
                print(f"  {j}. Title: {article.title}")
                print(f"     URL: {article.url}")
                print(f"     Content Length: {len(article.content)} characters")
                print(f"     Content Preview: {article.content[:200]}...")
                
                # Check for extraction failure marker
                if "Î≥∏Î¨∏ Ï∂îÏ∂ú Ïã§Ìå®" in article.content:
                    print(f"     ‚ö†Ô∏è  Contains 'Î≥∏Î¨∏ Ï∂îÏ∂ú Ïã§Ìå®'")
                else:
                    print(f"     ‚úÖ Content extraction successful")
                print()
        else:
            print("‚ùå No articles fetched")
        
        print()
        
        # Step 3: Summarization test (if articles available)
        if articles:
            print("üìù STEP 3: Summarization Testing")
            print("-" * 60)
            
            try:
                # Test with first 2 articles
                test_articles = articles[:2]
                summaries = summarizer.summarize_articles(test_articles)
                
                print(f"üìä Articles Sent for Summarization: {len(test_articles)}")
                print(f"üìã Summaries Generated: {len(summaries)}")
                
                if summaries:
                    for j, summary in enumerate(summaries, 1):
                        print(f"  {j}. Title: {summary.get('title', 'Unknown')}")
                        print(f"     Summary Length: {len(summary.get('summary', ''))} characters")
                        print(f"     Summary Preview: {summary.get('summary', '')[:150]}...")
                        print()
                else:
                    print("‚ùå No summaries generated")
                    
            except Exception as e:
                print(f"‚ùå Summarization error: {e}")
        else:
            print("‚è≠Ô∏è  STEP 3: Skipped (no articles to summarize)")
        
        # Store results
        all_results[feed_url] = {
            'articles_fetched': len(articles),
            'articles': articles,
            'feed_parseable': len(feed.entries) > 0 if 'feed' in locals() else False,
            'bozo_flag': feed.bozo if 'feed' in locals() else True
        }
        
        print("=" * 80)
        print()
    
    # Final summary
    print("üéØ FINAL RESULTS SUMMARY")
    print("=" * 80)
    
    total_articles = 0
    successful_feeds = 0
    
    for feed_url, results in all_results.items():
        articles_count = results['articles_fetched']
        total_articles += articles_count
        
        if articles_count >= 2:
            successful_feeds += 1
            status = "‚úÖ SUCCESS"
        elif articles_count > 0:
            status = "‚ö†Ô∏è  PARTIAL"
        else:
            status = "‚ùå FAILED"
        
        print(f"{status} - {feed_url}")
        print(f"   üìä Articles: {articles_count}")
        print(f"   üö® Bozo: {results['bozo_flag']}")
        print()
    
    print(f"üìà OVERALL STATISTICS:")
    print(f"   üéØ Target: At least 2 articles per feed")
    print(f"   ‚úÖ Successful Feeds: {successful_feeds}/3")
    print(f"   üìä Total Articles Fetched: {total_articles}")
    print(f"   üìã Average per Feed: {total_articles/3:.1f}")
    
    if successful_feeds >= 2:
        print(f"üéâ GOAL ACHIEVED: {successful_feeds} feeds successfully providing 2+ articles each!")
    else:
        print(f"‚ö†Ô∏è  GOAL NOT MET: Only {successful_feeds} feeds met the 2+ article requirement")
    
    print("=" * 80)

if __name__ == "__main__":
    test_korean_rss_comprehensive() 